[
{
	"uri": "/2-runtask/2.1-awsbatch/",
	"title": "AWS Batch Design",
	"tags": [],
	"description": "",
	"content": "In this step, we will set up a complete AWS Batch to run the job. To use AWS Batch, you need to create the components that you mentioned in the previous article:\nCompute Environments Job Queues Job Definitions Batch Wizard For more convenience, it is better to use the wizard instead of creating individual components. Go to AWS Batch and select Wizard. If you want more options, you can use EC2, or EKS if you use Kubernete. But for convenience, I\u0026rsquo;ll use Fargate. Compute Environment Select a name for compute env Spot Instances can help save costs, but tasks running on spot instances can be interrupted in the middle, only if the task is short or can be replaced by other jobs. I will choose not in this post. Also select the maximum vCPU, it is possible to leave 4 vCPUs. Go to the Network Configuration step, you can set up as follows\nCreate a **VPC with a public subnet Make sure to create an Internet Gateway and Route table for Subnet to access the internet Create a security group for compute env, open outbound traffic to the internet (default is open so no need to edit) Add created sections to compute env Select next to create a compute env Job Queue Choose a name for the job queue Set **Priority to 1. This is the priority order, in case you have multiple job queues in the same compute env, the high priority job queue will be run first. Job Definition Select the name and timeout for the job definition. Timeout** is the maximum running time of a job. If the job runs beyond this time, it will stop. I estimate my job takes less than 15 minutes, so I leave it 15 minutes (900s). Fargate, you leave **version as LATEST, turn on Public IP because the job needs access to the Internet. Ephemeral storage is the capacity of Fargate, to range from 21GB to 200GB. Execution roles to give your task access to the ECR to pull images from there, read Secret from Secret Manager and some low-level roles. In the container configuration, in the image section, you need to go to ECR, copy the URI of the ECR repo that you created earlier Paste in the Image section of the Batch Wizard and add latest to the back** so you can get the latest image The next part is command. You can add commands to your Docker Image. This command will **not overwrite CMD or ENTRYPOINT in your docker, but will add. Because I don\u0026rsquo;t need anything else, I\u0026rsquo;ll leave it default, the default command just to print Hello World It seems odd to have two roles in the job definition section, however, follow [explanation] (https://repost.aws/questions/QU73a6ZU6RQg-CNJ-25i_i_Q/which-role-do-i-have-to-use-for-the-fargate-tasks-on-aws-batch) here. If your task needs higher level permissions such as S3 access (permissions that not all tasks need), you can leave it in job role, while lower permissions such as pull image from ECR can be fixed in **execution role. Your task needs access to S3 so it will create a Full Access S3 role, but if you don\u0026rsquo;t need anything but run the task, you can go blank (select None). At the same time you need to select vCPU and Memory Choose a name for your job. When running, this name will be displayed. Once you\u0026rsquo;re done, select Review and Create Resource. So you have completed the AWS Batch setup. To check, you can go to Job Definition and select Submit New Job\n"
},
{
	"uri": "/1-cicd/1.1-setupgithubandecr/",
	"title": "Github Repo Preparation ",
	"tags": [],
	"description": "",
	"content": "In this article, you will prepare the necessary files for the following articles. I\u0026rsquo;ll go through each section. In this lab, I will use VsCode to prepare all files in the local environment and push to Github. I also use the console to deploy the architecture to the cloud. In the second part of the workshop, I will show how you can deploy with CDK (Cloud Development Kit) if you want to learn more about IaC.\nGetting Crawler and Push to Github First of all, you need to prepare the crawler file and push it to Github. This will be the crawler that we will deploy to the cloud. Please go to the following Github link: [Github Crawler] (https://github.com/MinhThieu145/Job-Scraper-Home.git). After entering, clone this repo to your computer. You can clone by running the following command:\ngit clone -b indeed-scraper https://github.com/MinhThieu145/Job-Scraper-Home.git If you have an error in the above statement, try using:\ngit --version If you see the results returned clearly show the git version. You have successfully installed Git. If not, you need to install git before clone repo. You can install git by referring to the following link [How to install Github for VsCode] (https://www.geeksforgeeks.org/how-to-install-git-in-vs-code/). Once the clone is complete, you will have a repo as follows:\n``markdown üì¶ Job-Scraper-Home üìÇ .git üìÇ pycache üìÇ result üìú .gitignore üìú buildspec.yml üìú dockerfile üìú exploration.ipynb üìú job_description_analyzer.py üìú main.py üìú requirements.txt üìú scraper.py\nI will go through each file and folder that you need to know in this repo: ### 1. main.py and scraper.py The first is the**main.py** file. The file runs the scraper.py file and contains the results in a folder named results (which will create a new folder if it doesn\u0026#39;t exist). Finally, the script will push all the files in the results folder to S3. ``python # import indeed scraper import scraper # import analyse data function import job_description_analyzer # some other libraries import os import pandas as pd # main function def main (): # clean the result folder # get current dir current_dir = os.getcwd () # get the result folder if exit try: # if the result folder exists result_dir = os.path.join (current_dir, \u0026#39;result\u0026#39;) # delete all the files in the result folder for file in os.listdir (result_dir): os.remove (os.path.join (result_dir, file)) except: # create a new folder called result os.mkdir (\u0026#39;result\u0026#39;) result_dir = os.path.join (current_dir, \u0026#39;result\u0026#39;) # run the scraper scraper.main () # get the result from a folder called result # loop through the result folder for file in os.listdir (result_dir): # if that is a csv file if file.endswith (\u0026#39;.csv\u0026#39;): # read with pandas df = pd.read_csv (os.path.join (result_dir, file)) # if the dataframe is not empty if not df.empty: # pass that to the analyse data function job_description_analyzer.main (df=df, df_name=file.split (\u0026#39;.\u0026#39;) [ 0]) if __name__ == ‚Äú__main__‚Äù: main () If you read the code carefully, you will see main.py loop through all the files that end .csv in the results folder and call another function in job_description_analyzer.py before pushing to S3. The reason is that I cut the repo a lot short. Initially, the crawler will have the following tasks.\nmain.py will go to S3 to find crawlers (each website is a different crawler) and run these crawlers. After all, these crawlers will save the results in the results folder. - After that, the job_description_analyzer.py file is responsible for extracting keywords and important information using the GPT API, before saving the results again Finally, both initial and post-processing scrap results are pushed to S3. Because this process is quite long, perishable and inefficient when almost completely processed by code. If you want to try it again, you can use AWS Step Function and AWS SNS to decouple parts, to ensure execution order and pipeline stability. In this lab, I\u0026rsquo;m not going to go through that part.\n2. dockerfile and requirements.txt These are the two files needed to build a docker image from code. In File DockerFile, I did\nUse the based image as Ubuntu:22.04. But you can use other base images, as long as you have Python3 preset. Otherwise you will have to install Python3 yourself Install and update pip to download necessary packages for crawler. The names of these packages are stored in the file requirements.txt. In this lab, I used boto3 to be able to call AWS APIs, openai to be able to call API of GPT3 (can be abandoned because it is no longer used), pandas to process data, selenium to be able to run crawler and webdriver_manager to be able to automatically load drivers for selenium. All of these packages are stored in the requirements.txt file. You can add or remove these packages as you wish. ``r boto3==1.26.165 openai==0.27.7 pandas==1.5.3 selenium==4.10.0 tenacity==8.2.2 webdriver_manager==3.8.6\n- Install Chrome browser for images. - Finally, copy the files into the docker image You can read your dockerfile here ``dockerfile # based image: Ubuntubased. BTW, for the PYTHON:3.9 like you used last time. It used the Debianbased image FROM public.ecr.aws/docker/library/ubuntu:22.04 # install a few things RUN apt-get update \u0026amp;\u0026amp; apt-get install -y\\ bash\\ git\\ curl\\ software-properties-common\\ pip\\ \u0026amp;rm -rf /var/lib/apt/lists/ * # workdir WORKDIR /srv # okay now pip RUN apt-get -y update RUN pip install --upgrade pip # Copy the requirements.txt file first, for separate dependency resolving and downloading COPY requirements.txt. RUN pip install -r requirements.txt # Install chrome broswer RUN curl -SS -o - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - RUN echo ‚Äúdeb http://dl.google.com/linux/chrome/deb/ stable main‚Äù \u0026gt;\u0026gt; /etc/apt/sources.list.d/google-chrome.list RUN apt-get -y update RUN apt-get -y install google-chrome-stable # add the main.py COPY main.py. # add scraper COPY scraper.py. # add job_description_analyzer COPY job_description_analyzer.py. ENTRYPOINT [‚Äúpython3\u0026#34;,\u0026#34; main.py \u0026#34;] 3. buildspec.yml There is one more file that you need to pay attention to, which is the buildspec.yml file. This file is extremely important, as it shows AWS Codebuild how to build your image. Buildspec is divided into three phases, pre_build, build and post_build.\n``yaml Version: 0.2\nPhases: pre_build: commands:\necho ‚ÄúLogging in to ECR‚Äù aws \u0026ndash;verzija AWS_default_region=US-East-1 AWS_ACCOUNT_ID=238101178196 image_tag=Latest repository_uri=238101178196.dkr. ecr.us-east-1.amazonaws.com /indeed-scraper aws ecr get-login-password \u0026ndash;region $AWS_DEFAULT_REGION | docker login \u0026ndash;username AWS \u0026ndash;password-stdin $aws_account_id.dkr.ecr. $AWS_Default_Region.amazonaws.com build: commands:\necho ‚ÄúBuilding the Docker image\u0026hellip;‚Äù docker build -t $REPOSITORY_URI: $IMAGE_TAG. docker push $REPOSITORY_URI: $IMAGE_TAG post_build: commands:\necho ‚ÄúUpdating ECS service\u0026hellip;‚Äù echo ‚ÄúEnd of script‚Äù artifacts: filer:\n\u0026lsquo;**/*\u0026rsquo; name: artifacts In phase **pre_build**, I saved the environment variables as follows: - **AWS_DEFAULT_REGION: ** region that you use for this workshop, I use us-east-1 (virginia) - **AWS_ACCOUNT_ID: ** Your AWS Account ID, which can be found in the account section ! [ lol] (/images/2023-07-09-09-52-36.png) - **IMAGE_TAG**: The tag of the image you want to get in ECR. During the build process, you will update the image several times, each time with a tag + 1. The latest tag will help you get the latest image. - **REPOSITORY_URI: ** The link of ECR repo contains the image, I will create right at the bottom. - The last line of the phase is for me to authenticate my script, which allows me to execute commands in Docker such as Push or Pull image from Ecr. In phase **build, we will build the image and push it to the ECR repo. - `docker build -t $REPOSITORY_URI: $IMAGE_TAG .` I build image and set tag as **latest**. This is the IMAGE_TAG variable I set before. Remember that there is a dot after IMAGE_TAG. - `docker push $REPOSITORY_URI: $IMAGE_TAG` I push Docker image I just built to Ecr repo. In this paragraph, if you follow me, you will be stuck because you haven\u0026#39;t created a repo, so you can leave this variable blank, and read on to learn how to create a repo. Finally, the post_build phase. I just log to the console to announce it is complete. This phase does not execute orders at all. ## Deploy Github repo to Github. After completing the preparation of the files, the next step is to push the repo to Github. First, you need to open the folder you just cloned the repo to. In VsCode it will look like this ![](/images/2023-07-19-18-32-06.png) 1. Check if you already have git with `git --version`. Next, you need to remove the **.git file from the folder as the folder is still associated with your Github repo. Use this command to remove the file .git `rm -rf .git`. Then you need to create a new repo on your Github. 2. `git init -b main` to initialize the new repo. 3. `git add .` to add all files in the folder to the repo 4. `git commit -m ‚ÄúInitial commit\u0026#34;` to commit files just added to the repo 5. Go to your github account, create a new repo and copy the URL 6. `git remote add \u0026lt;URL\u0026gt;origin` to add new repo to your local repo 7. `git push -u origin main` to push repo to Github. If you fail, try `git push -f origin main` to force push. 8. After the push is done, you can see that your repo has been updated "
},
{
	"uri": "/",
	"title": "Introduction and Preparation",
	"tags": [],
	"description": "",
	"content": "General Introduction Welcome to Workshop 1. In this workshop, I will design a simple crawler architecture so that you can crawl job postings on Indeed. This lab will be divided into three main parts:\n1. CI/CD Pipeline Design Part 1 of the lab will be about the CI/CD process of crawler. I will use CodeBuild to automatically get code from Githubm to build Docker image for crawler and automatically push the image into ECR. The architectural part will design as follows:\n2. Architectural design for crawler Part 2 of the lab, which is also the main part, will be about running the crawler as a Docker image using Fargate. I will also provide code for crawlers. Crawler is written in Python and Selenium. You can also expand to create crawlers for other websites or modify existing crawlers as you like. The architecture includes AWS EventBridge to be able to run a scheduled crawler, AWS Lambda to send job definition to the job queue in the AWS Batch. Once completed, the results will be stored in S3. The design architecture is as follows:\n3. Design a front-end webapp to display the results Part 3 of the lab will focus on designing a high-availability webapp to display crawler results. I will use Streamlit and write in Python. The webapp will be placed in EC2. The design architecture is as follows: "
},
{
	"uri": "/3-frontendapp/3.1-vpc/",
	"title": "Network Environment Design",
	"tags": [],
	"description": "",
	"content": "The first step is to design a network environment for the app. As mentioned earlier, here are the resources we will create:\n2 public subnets, 2 private subnets 1 VPC 1 Internet Gateway 1 NAT Gateway (can create 2 NAT Gateways if you want to be more secure - each NAT in 1 subnet) S3 Endpoint to access S3 without going through NAT Gateway Setup VPC and Subnet Select VPC, Create VPC. Select the option VPC and more**. Name the VPC. You can defaul CIDR: 10.0.0.0/16, you can also replace CIDR to increase and decrease the number of IPs in VPC. You can leave IPV6 as default (not using IPV6) **Number of Availability Zones: ** the number of AZ that the VPC will be generated. According to the diagram, I will leave 2 AZ. You can also choose Customize AZs to select other AZ (default is us-east-1a and us-east-1b if you use us-east-1) **Number of public subnets: ** How many public subnets in VPC? I need 2 public subnets (each AZ 1 public subnet) so I will leave 2. The graph on the right also shows each part of the public subnet: each AZ has a subnet, the subnet is passed through the Route table to connect to the Internet Gateway. **Number of private subnets: How many private subnets in VPC? I also need 2 private (1 AZ 1 private). If you choose 4, you can see the number of private subnets increase by 1 per AZ **Customize subnets CIDR blocks: ** You can adjust this parameter to increase and decrease the number of IP addresses per subnet. Currently, each submet has 4096 IPs **NAT gateways ($) ** This is quite expensive part. If you want to be safe and according to the diagram, you can choose 1 per AZ so that each AZ has 1 NAT. You can also select In 1 AZ to create only one NAT gateway for both private subnets.\nVPC endpoints Should be selected to reduce S3 access costs. If you use boto3 without an Endpoint, you have to forcibly pass through NAT (due to data being transmitted to the internet). Having Endpoint will reduce the cost of NAT (if you don\u0026rsquo;t really need the Internet) and data transfer fees.\nDNS options** choose both options\nSelect Create VPC to complete.\n"
},
{
	"uri": "/1-cicd/",
	"title": "Preparation and design of CI/CD Pipeline",
	"tags": [],
	"description": "",
	"content": "General Introduction Welcome to the first part of the workshop. In this section, I will go through step by step to prepare crawler and push to Github, design CodeBuild to be able to automatically build and deploy docker images to AWS ECR\n"
},
{
	"uri": "/1-cicd/1.2-setupecrrepo/",
	"title": "Setup repo cho AWS ECR",
	"tags": [],
	"description": "",
	"content": "Congratulations on getting your Github repo ready. This will repo to contain your crawler and use it in the CI/CD pipeline later. In this article, I will create an AWS ECR repo to contain the image after the build from Codebuild. *AWS ECR service at a glance: * This is an AWS service that allows the management, storage, and deployment of containers. This service is used with services running Conatiner such as AWS ECS, AWS EKS, AWS Fargate, AWS Batch, etc. In this workshop, I use AWS ECR along with AWS Batch\nCreate a repo on AWS ECR Go to AWS and find AWS ECR -\u0026gt; Create repository. The settings you set are as follows\n**Visibility: ** Select private mode. I have a bug that I can\u0026rsquo;t pull an image if it\u0026rsquo;s public, so I recommend setting it to private **Repository name: ** Name the repo, you can see the repo link. In the Repository name section, you will see a place to enter the name. The rest is the URI that leads to your repo. I will name the repo as indeed-scraper, so the link to my repo will be: ``yaml 238101178196.dkr. ecr.us-east-1.amazonaws.com /indeed-scraper\nYou can see the link split into several parts. The leading number `238101178196` is your account ID. After the word `ecr` is the region used. I use the region as **us-east-1**. Finally the name you set yourself, yours is `indeed-scraper` In the other three options, you can leave it disabled. ![](/images/2023-07-09-10-07-01.png) Once created, you can copy the URI of the newly created repo and paste it into the **buildspec.yml** file in step 2. This is the part I told you to leave it empty because I haven\u0026#39;t created a repo. ![](/images/2023-07-09-10-14-49.png) "
},
{
	"uri": "/6-cdk/6.1-cicd/",
	"title": "CI/CD Pipeline Design",
	"tags": [],
	"description": "",
	"content": "In the previous lab, you learned how to create crawler pipelines with Codebuild. In this section, I will use CDK to do the same. In the repo that you clone, in the folder workshop1 there is a python file: cicd_stack.py. This is the CI/CD pipeline setup file for crawler. I\u0026rsquo;ll go through each section of this file. The first is an overview of the file: ``python from ws_cdk import (\nDuration, Stack,\naws codebuild aws_codebuild as codebuild,\naws iam aws_iam as iam, ) from constructs import Construct\nSome pther lib import os\nload the data for the CodeBuild from environment variables from dotenv import load_dotenv load_dotenv ()\nGITHUB_OWNER = os.getenv (‚ÄúOWNER‚Äù) GITHUB_REPO = os.getenv (‚ÄúREPO‚Äù) GITHUB_BRANCH = os.getenv (‚ÄúBRANCH‚Äù)\nplease use this line to add credential to the CodeBuild aws codebuild import-source-credentials \u0026ndash;server-type GITHUB \u0026ndash;auth-type PERSONAL_ACCESS_TOKEN \u0026ndash;token \u0026lt;token_value\u0026gt; The personal token needs to have enough permission to access the repo CICDStack class (Stack):\ndef init (self, scope: Construct, construct_id: str, **kwargs) -\u0026gt; None: super () .init (scope, construct_id, **kwargs)\nCreate the CodeBuild Project Create Github Source github_source = codebuild.source.git_hub ( owner=f \u0026ldquo;{GITHUB_OWNER}‚Äù, repo=f \u0026ldquo;{GITHUB_REPO}‚Äù, webhook=True, branch_or_ref=f \u0026ldquo;{GITHUB_BRANCH}‚Äù,\nthere are an option called webhook_filters: that is for the Webhook Event Filter on AWS. Leave it as default )\nCreate the role for the CodeBuild Project codebuild_role = iam.role ( scope=self, id=\u0026ldquo;JobScrapingCicDrole‚Äù, role_name=\u0026ldquo;JobScrapingCicDrole‚Äù, Assumed_by=Iam.servicePrincipal (‚Äúcodebuild.amazonaws.com‚Äù),\nadd the full S3 access to the role managed_policies= [ IAM.ManagedPolicy.From_AWS_Managed_Policy_Name ( managed_policy_name=\u0026ldquo;Amazons3fullAccess‚Äù ),\nadd full ECR access to the role IAM.ManagedPolicy.From_AWS_Managed_Policy_Name ( managed_policy_name=\u0026ldquo;Amazonec2ContainerRegistryFullAccess‚Äù ),\n]]\n)\nCreate the CodeBuild Project CodeBuild.project ( scope=self, id=\u0026ldquo;cicdproject‚Äù, project_name=\u0026ldquo;cicdproject‚Äù,\nadd description Description=\u0026ldquo;CodeBuild Project for the CICD for Job Scraping Project‚Äù,\nset the source source=github_source,\nadd some environment variables environment=CodeBuild.BuildEnvironment (\nadd the Base Environment Variable build_image=codebuild.linuxbuildimage.standard_7_0,\nset the privilege to true for docker build privileged=True, ),\nCreate role for the CodeBuild Project role=codebuild_role, )\n## Preparation of environment variables for Codebuild. Instead of connecting directly to Github as you use the console, you must use the variable environment to pass information into Codebuild. I will use the **dotenv library to load the environment variables from the **.env** file, but you can leave it in the code. ``python # load the data for the CodeBuild from environment variables from dotenv import load_dotenv load_dotenv () GITHUB_OWNER = os.getenv (‚ÄúOWNER‚Äù) GITHUB_REPO = os.getenv (‚ÄúREPO‚Äù) GITHUB_BRANCH = os.getenv (‚ÄúBRANCH‚Äù) In addition, I also import the necessary libraries for the stack ``python from ws_cdk import (\nDuration, Stack,\naws codebuild aws_codebuild as codebuild,\naws iam aws_iam as iam, ) from constructs import Construct\nSome pther lib import os\nload the data for the CodeBuild from environment variables from dotenv import load_dotenv load_dotenv ()\nAnother very important step for you to run Gitbuild is to grant access to Github to AWS. You need to get your personal access key from Github and use the following command. ``bash aws codebuild import-source-credentials --server-type GITHUB --auth-type PERSONAL_ACCESS_TOKEN --token \u0026lt;token_value\u0026gt; When creating personal access from Github, you need to give AWS permissions. If it fails, it is necessary to check the rights of the token\nCreate Github Source In the console, select Source from the console as shown in the image: But in CDK, you need to create Github Source with code. You can find out more at [Github Source] (https://docs.aws.amazon.com/cdk/api/latest/python/aws_cdk.aws_codebuild/Source.html#aws_cdk.aws_codebuild.Source.git_hub).\n``python\nCreate Github Source github_source = codebuild.source.git_hub ( owner=f \u0026ldquo;{GITHUB_OWNER}‚Äù, repo=f \u0026ldquo;{GITHUB_REPO}‚Äù, webhook=True, branch_or_ref=f \u0026ldquo;{GITHUB_BRANCH}‚Äù,\nthere are an option called webhook_filters: that is for the Webhook Event Filter on AWS. Leave it as default )\n## Create Environment In the console, in the Environment tab, you need to - Select Operating System (I use Ubuntu) - tick privilege to allow docker build - Create a new role or select an existing one ![](/images/2023-07-20-02-12-58.png) In CDK, to create a new role ``python # Create the role for the CodeBuild Project codebuild_role = iam.role ( scope=self, id=\u0026#34;JobScrapingCicDrole‚Äù, Role_name=\u0026#34;JobScrapingCicDrole‚Äù, Assumed_by=Iam.servicePrincipal (‚Äúcodebuild.amazonaws.com‚Äù), # add the full S3 access to the role managed_policies= [ IAM.ManagedPolicy.From_AWS_Managed_Policy_Name ( managed_policy_name=\u0026#34;Amazons3fullAccess‚Äù ), # add the full ECR access to the role IAM.ManagedPolicy.From_AWS_Managed_Policy_Name ( managed_policy_name=\u0026#34;Amazonec2ContainerRegistryFullAccess‚Äù ), ]] ) After that, you can create a Codebuild Project. Parts like operating system, tick privilege can be done here. You can see in the code below, I named it, description, assigned the source as github, created the environment as Linux Standard 7.0 similar to the console, tick privilege and choose the role as the role just created.\n``python\nCreate the CodeBuild Project CodeBuild.project ( scope=self, id=\u0026ldquo;CicdProject‚Äù, Project_name=\u0026ldquo;cicdProject‚Äù,\nadd description description=\u0026ldquo;CodeBuild Project for the CICD for Job Scraping Project‚Äù,\nset the source source=github_source,\nadd some environment variables environment=CodeBuild.BuildEnvironment (\nadd the Base Environment Variable build_image=codebuild.linuxbuildimage.standard_7_0,\nset the privilege to true for docker build privileged=True, ),\nCreate a role for the CodeBuild Project role=codebuild_role, )\nSo you have completed the CI/CD. "
},
{
	"uri": "/1-cicd/1.3-deployoncloud/",
	"title": "Deploy CI/CD pipeline to AWS",
	"tags": [],
	"description": "",
	"content": "After completing the preparation steps, you also have all the files you need to deploy your CI/CD pipeline to AWS. In this article, we\u0026rsquo;ll show you how to deploy CI/CD pipelines to AWS using AWS Codebuild\n*AWS Codebuild Services at a glance: * AWS Codebuild allows you to automatically build, test, and containerize your code. Codebuild can be used with AWS CodePipeline to automatically deploy after build. However, in this article, I just build and push the image to ECR.\nCreate Codebuild Project Find Codebuild in AWS and select Create Build Project** In the Project configuration section, enter the project name and description. In the Source section, select Github as the provider In the Repository section, select Connect to Github** and select the repo you created in the previous step. You may be asked to connect to Github at this step, enter your password into the connection. Next to Primary source webhook events, tick webhook options to build with webhook, select single build and event type PUSH In the Environment section, select \u0026lsquo;Managed image** (Docker image already includes Operating System, so this is not necessary. But if you don\u0026rsquo;t use Docker and need some packages, you can use Custom Image.) Operating System select Ubuntu, Runtime is Standard and Version is latest 7.0. Environment type* choose Linux without GPU. You must select Privileged in the Additional Configuration section. If not selected, it will not be possible to push the image to ECR.\nChoose a new role. In the Buildspec section, select Use a buildspec file, and leave the name box blank, because you only need to rename the file without leaving the file in the root folder or using a different name. Leave the rest as is and select Create Build Project. So you have completed the steps to have a pipeline that automatically retrieves code from Github, buil docker image and push to ECR repo.\n"
},
{
	"uri": "/2-runtask/",
	"title": "Designing AWS Batch and S3 to run crawlers and save results",
	"tags": [],
	"description": "",
	"content": "Overview Introduction Congratulations on completing the first part of the workshop. In this section, I will design AWS Batch to be able to automatically retrieve the images from the ECR repo that I made in the previous section and run according to the existing schedule. I will also create an S3 bucket to save the results. The services that I will use in this post:\n**AWS Batch: ** To run your task crawler **AWS S3: ** To store results **AWS EventBridge: ** To run scheduled tasks. If you only use AWS Batch, you will have to submit the job manually. So I will use EventBridge to automatically submit jobs to Batch according to the available schedule **AWS Lambda: ** However EventBridge can set schedules. However, for more features, I will use Lambda to submit jobs to Batch. Lambda will be triggered by EventBridge very easily. And Lambda itself, I\u0026rsquo;ll use boto3 to connect to the batch and submit the job. Introduction to AWS Batch AWS Batch is a serverless service that allows you to run batch jobs. AWS Batch will automatically scale smaller nodes. Helps you save money and run jobs faster. We\u0026rsquo;ll go through each section of AWS Batch:\n**Compute Environment: ** This is the place for you to choose the compute resource that suits your needs. Since Batch is a serverless service, you won\u0026rsquo;t have to setup too much **Job queue: ** 1 queue to contain jobs. When you submit jobs to the batch, the jobs will be in the queue Job definition: ** is the definition of the job. You can imagine this is the drawing of the job you want to make. When you want to do this job definition, you submit it to the queue. About S3 This is a well-known AWS service. S3 is Object Storage Service S3 can save different file formats, code, video or images, etc.\n"
},
{
	"uri": "/3-frontendapp/3.2-elb/",
	"title": "Generate resources: ALB and ASG",
	"tags": [],
	"description": "",
	"content": "After completing the process of creating a VPC, you need to create additional resources in the VPC. In this article, we will create the following resources:\nLaunch Template: This is like 1 drawing. Instace created from ASG will resemble this drawing. And to create a Launch Template with all the necessary packages, I will create an AMI *Amazon Machine Image (AMI) at a glance: * AMI can be understood as a drawing of an instance. When creating a new EC2 instance, you are asked to select AMI. Most of you will use base image, AMIs provided by AWS such as ubuntu, linux, or window. However, if we want EC2 to be created with existing packages, we can create a custom image.\nASG: Create ASG using the Launch Template above ALB: Create ALB to navigate request Create Custom AMI and Launch Template When instances are initialized from ASG, they need a template. To create a launch template, we first need to create a custom AMI with all the necessary packages.\nGo to EC2, select Launch Template** and then select Create launch template** Name the template and select the option Auto Scaling Guidance for convenient integration with ASG later. Select AMI and click Browse more AMIs Enter the number 238101178196 in the search bar and select Community AMI, you should see an AMI with the name shown below. This is my public AMI, curing the files running the server. If you follow the previous steps, especially the bucket naming of S3, you can use it without any changes. Inside AMI, there is a streamlit server with port 8501, and saves the results to S3 in Bucket named job-description-process, and retrieved from the folder named indeed-scraper\nSelect that AMI, and you will be taken back to the template to continue. In the ‚ÄúInstance Type‚Äù section, select t2.micro. In the Key Pair section, select a key pair that you already have so that later it is convenient to SSH into the instance (of course, through Bastion Host) The Network Settings section you can leave blank (do not select anything) You can leave the rest default.\nIn Advanced details**, expand and select Create new IAM profile, because the Frontend app needs a few roles (Access S3 and read EC2). Create a new role with the following policies Once created, go back to the template, reload (the round arrow next to it) to see the created role and select Then select Create launch template** to create the template.\nCreate Auto Scaling Group Go to EC2, scroll down to the last to see the ALB. Select Create Auto Scaling Group and choose a name for ASG. Then in the Launch Template** section, select the Template you created earlier, choose the version as Latest In the Network tab, select the VPC you created earlier. In that VPC, select both Private Subnet** that you created earlier. We will place the application in Private Subnet to increase security, Public Subnet only to create NAT Gateway and set Bastion Host. Click Next In the Load Balancing tab, select Attach to new load balancer. In the newly opened tab, select Application Load Balancer, select a name for Load Balancer, and select **Internet-facing Next is the network mapping section, the VPC will remain the same as that of ASG (of course, ELB to handle traffic into ASG, so it needs to be the same VPC). You need to select 2 Public Subnet** to set the ELB In the Listener and Routing section, select Port as 8501 for Streamlit, and select Create a target group and name the target group In the Health checks section, select Turn on Elastic Load Balancing Health Checks. Also set the Health Check Grace Period to 300s. This is the estimated time it takes for EC2 to start (this is a way for the EC2 instance to avoid being destroyed when it is not finished booting. If User Data causes the instance to take startup time, estimate the time it takes to boot here). Leave the Additional settings** section as default and click Next\nGroupsize consists of three parts: **Maximum capacitance for maximum instance, **Minimum capacity is the minimum number of instances, **Desired capacity is the number of instances that ASG tries to hold at that level. Leave the Scaling Policies section blank and select Next\nSkip the Add Notifications** section and select Next. Skip tags and next and select Create Auto Scaling Group\n"
},
{
	"uri": "/6-cdk/6.2-runtask/",
	"title": "AWS Batch Design",
	"tags": [],
	"description": "",
	"content": "In the previous section, after I finished designing the CI/CD pipeline, I continued with AWS Batch. In this section, I\u0026rsquo;ll go through each step of designing AWS Batch using CDK, including\nCreate a role for the batch. In the AWS Batch Console, there is a framework called Execution Role. I need to create and assign this role to the batch. Design of VPC network infrastructure for Batch Create Security Group for Batch Create the remaining components for the batch: Compute Env, Job Definition, Job Queue, etc. Create EventBridge to be able to Schedule for Batch to run on schedule If you wonder why there is no bucket. In this workshop, I created a manual bucket. The reason is that passing bucket name into script crawler is quite difficult due to the way I write the script. Therefore, I will have to create a bucket, pass the bucket name into the crawler script, and then be able to run the crawler.\n``python from ws_cdk import (\nDuration, Stack,\niam aws_iam as iam,\naws batch aws_batch as batch,\nlambda aws_lambda as aws_lambda,\nDuration Duration,\naws events aws_events as events,\naws events targets aws_events_targets as targets,\naws ec2 (for vpc) aws_ec2 as ec2,\n) from constructs import Construct\nOther lib import os\nload the environment variables from dotenv import load_dotenv load_dotenv ()\nload ECR_REPO_NAME = os.getenv (‚ÄúECR_REPO‚Äù) JOB_DEFINITION_NAME = os.getenv (‚ÄúJOB_DEFINITION‚Äù) JOB_QUEUE_NAME = os.getenv (‚ÄúJOB_QUEUE‚Äù) JOB_NAME = os.getenv (‚ÄúJOB_NAME‚Äù) JOB_ROLE_NAME = os.getenv (‚ÄúECS_ROLE_NAME‚Äù) BATCH_VPC_NAME = os.getenv (‚ÄúBATCH_VPC_NAME‚Äù) BATCH_SUBNET_NAME = os.getenv (‚ÄúBATCH_SUBNET_NAME‚Äù) BATCH_SG_NAME = os.getenv (‚ÄúBATCH_SG_NAME‚Äù)\nRunTaskStack class (Stack):\ndef init (self, scope: Construct, construct_id: str, **kwargs) -\u0026gt; None: super () .init (scope, construct_id, **kwargs)\naws batch For the simplicity of this, let create a role for ECS job ecs_role = iam.role ( scope=self, id=f \u0026ldquo;{JOB_ROLE_NAME}‚Äù, assumed_by=Iam.servicePrincipal (\u0026rsquo;ecs-tasks.amazonaws.com\u0026rsquo;), role_name=f\u0026rsquo; {JOB_ROLE_NAME} \u0026lsquo;, managed_policies= [ IAM.MANAGEDPOLICY.FROM_AWS_MANAGED_POLICY_NAME (\u0026lsquo;SERVICE-ROLE/AMAZONECSTASKExecutionRolePolicy\u0026rsquo;),\nadd full access to S3 IAM.MANAGEDPOLICY.FROM_AWS_MANAGED_POLICY_NAME (\u0026lsquo;AMAZONS3FULLACCESS\u0026rsquo;),\nadd full access to ECR iam.managedpolicy.from_aws_managed_policy_name (\u0026lsquo;Amazonec2ContainerRegistryFullAccess\u0026rsquo;), ], )\n+++ Create a VPC for the Batch +++ vpc = ec2. Vpc ( scope=self, id = f \u0026ldquo;{BATCH_VPC_NAME}‚Äù, vpc_name=f \u0026ldquo;{BATCH_VPC_NAME}‚Äù,\nset the max azs to 1 max_azs=1,\nset the cidr cidr=\u0026lsquo;10.0.0.0/16\u0026rsquo;,\nset the subnet configuration subnet_configuration= [ ec2. SubNetConfiguration ( name=f \u0026ldquo;{BATCH_SUBNET_NAME}‚Äù, subnet_type=ec2. Subnettype.public, cidr_mask=24, ), ],\n)\nCreate a Security Group for the Batch in the VPC batch_security_group=ec2. SecurityGroup ( scope=self, id=f\u0026quot;BatchSecurityGroup‚Äù, vpc=vpc, security_group_name=f \u0026ldquo;{BATCH_SG_NAME}‚Äù,\nallow all outbound traffic allow_all_outbound=True,\n)\ncreate compute environment compute_environment = batch.cfnComputeenVironment ( scope=self, ID=\u0026ldquo;JobScrapingComputeenVironment‚Äù, compute_environment_name=\u0026lsquo;job-scraping-compute-environment\u0026rsquo;,\nif you set the type as MANAGED, it will pass in the settings from Launch Template, otherwise you need to set the settings manually type=\u0026lsquo;managed\u0026rsquo;, state=\u0026lsquo;Enabled\u0026rsquo;,\nCompute Resources: to set the type of compute resource compute_resources=batch.cfnComputeenVironment.ComputeresSourcesProperty ( maxv_cpus=4, subnets= [vpc.public_subnets [0] .subnet_id], type=\u0026lsquo;Fargate_Spot\u0026rsquo;,\nSecurity Group security_group_ids= [batch_security_group.security_group_id], ),\nService Role Not Necessary service_role=batch_role.role_arn, )\ncreate job queue job_queue = batch.cfnJobQueue ( scope=self, id=f \u0026ldquo;{JOB_QUEUE_NAME}‚Äù, job_queue_name=f\u0026rsquo; {JOB_QUEUE_NAME} \u0026lsquo;, priority=1, state=\u0026lsquo;Enabled\u0026rsquo;, compute_environment_order= [ Batch.cfnJobQueue.computeenVironmentorderProperty ( compute_environment=compute_environment.ref, order=1, ) ], )\ncreate job definition job_definition = batch.cfnjobDefinition ( scope=self, id=f \u0026ldquo;{JOB_DEFINITION_NAME}‚Äù, job_definition_name=f\u0026rsquo; {JOB_DEFINITION_NAME} \u0026lsquo;,\ntimeout= { \u0026lsquo;AttemptDurationSeconds\u0026rsquo;: 900, },\ntype (so this is not multi-node) type=\u0026lsquo;container\u0026rsquo;,\nplatform_capabilities= [\u0026lsquo;FARGATE\u0026rsquo;],\ncontainer properties container_properties=batch.cfnjobDefinition.ContainerPropertiesProperty (\nTHIS IS ‚ÄúSOME‚Äù OF THE FARGATE PLATFORM CONFIGURATION. THE REASON I SAID SOME IS BECAUSE, NOT SURE WHY, THE SETTINGS ARE IN MANY PLACES assign public IP (does not find) Ephemeral storage epheemeral_storage=batch.cfnjobdefinition.ephemeralstorageProperty ( size_in_gib=30 # between 21 and 200 ),\nExecution role execution_role_arn=ecs_role.role_arn,\n++++ THIS IS SOME MORE FARGATE CONFIGURATION +++++ This is the latest that you see in the console fargate_platform_configuration= batch.cfnjobdefinition.fargateplatformConfigurationProperty ( platform_version=\u0026lsquo;latest\u0026rsquo;,\n),\nnetwork configuration (For public IP) Network_Configuration=Batch.cfnJobDefinition.NetworkConfigurationProperty ( assign_public_ip=\u0026lsquo;Enabled\u0026rsquo;, ),\nprivileged: Do not add this!!! Fargate actually ban this privileged=True, THIS IS ON THE TAB CONTAINER CONFIGURATION IN JOB DEFINITION (Step 2 if you use Console) This is the link to the ECR that we store our image (the image that we build, and create in the CICD step). Don\u0026rsquo;t forget to add the tag image=f\u0026rsquo; {ECR_REPO_NAME} :latest\u0026rsquo;,\nHonestly, this is not really useful, since I have already set the ENTRYPOINT in the Dockerfile command= [ \u0026rsquo;echo\u0026rsquo;, \u0026lsquo;Job Definition Initiated\u0026rsquo;, ],\njob role: Explain simply, job role is more specific than Execution role. All the jobs can have the same execution role, but different job roles job_role_arn=ecs_role.role_arn,\nmemory and vcpus resource_requirements= [ Batch.cfnJobDefinition.ResourceEquirementProperty ( type=\u0026lsquo;Memory\u0026rsquo;, value=\u0026lsquo;2048\u0026rsquo;, ),\nBatch.cfnjobDefinition.ResourceEquirementProperty ( type=\u0026lsquo;vcpu\u0026rsquo;, value=\u0026lsquo;1\u0026rsquo;, ),\n],\n),\n)\n+++ Create a Lambda to submit job to Batch +++ create a role for lambda to submit job to batch submit_job_lambda_role = iam.role ( scope=self, id=\u0026ldquo;SubmitBatchJobLambDarole‚Äù, role_name=\u0026ldquo;SubmitJobLambDarole‚Äù, Assumed_by=Iam.servicePrincipal (‚Äúlambda.amazonaws.com‚Äù),\nadd the job submit policy to the role managed_policies= [ IAM.ManagedPolicy.From_AWS_Managed_Policy_Name ( managed_policy_name=\u0026ldquo;Service-role/AWSBatchServiceEventTargetRole‚Äù ), ],\n)\ncreate a lambda function submit_job_lambda = AWS_Lambda.function ( scope=self, id=\u0026ldquo;SubmitJobLambda‚Äù, function_name=\u0026ldquo;cdk-jobscraping-submit-job-batch‚Äù, runtime=AWS_lambda.runtime.python_3_9, code=AWS_lambda.code.from_asset (‚ÄúWorkshop1/Lambda_Functions/TriggerjobScrapingTask/‚Äù), handler=\u0026ldquo;lambda_function.lambda_handler‚Äù, timeout=Duration.seconds (60),\npass in the role role=submit_job_lambda_role,\nenvironment= { ‚ÄúJOB_NAME‚Äù: JOB_NAME, ‚ÄúJOB_QUEUE‚Äù: JOB_QUEUE, ‚ÄúJOB_DEFINITION‚Äù: JOB_DEFINITION_NAME, }, )\n+++ Event Bridge Rule +++ create a rule to trigger the lambda function at 6am everyday rule = events.rule ( scope=self, id=\u0026ldquo;TriggerSubmitJobLambda‚Äù, rule_name=\u0026ldquo;TriggerSubmitJobLambda‚Äù, schedule=events.schedule.cron ( minute=\u0026ldquo;0\u0026rdquo;, hour=\u0026ldquo;6\u0026rdquo;, ), )\nadd the lambda function as the target rule.add_target (targets.lambdaFunction (submit_job_lambda))\n"
},
{
	"uri": "/3-frontendapp/",
	"title": "Create a Frontend Webapp to see the results",
	"tags": [],
	"description": "",
	"content": "Overview Introduction If you have made it here, congratulations on almost completing the workshop. In previous sections, Crawler was able to run on a schedule and save the results to S3. However, there is still no way you can view those results conveniently, or share them with others. In this article, I will continue to design a streamlit webapp to display results from S3.\nAbout Streamlit Streamlit is an open-source library that uses Python to build webapps. Streamlit is primarily used to share data science results. However, you can use it to build webapps for other purposes. Streamlit can run locally or in the cloud. In this post, I will run on EC2.\nArchitectural Design Because the structure is a bit long, I will go very carefully. First of all, read the diagram architecture of this section. I will go in the order in which the client will go (i.e. go from the client side to the server side). 1. Application Load Balancer First, the user will access the DNS of the Application Load Balancer (ALB) through the browser. ALB will redirect the request to the Auto Scaling Group, from which access the app is placed in the private subnet. The ALB itself must be placed in the public subnet, as it will receive requests from the internet. The app will be placed in the private subnet because it will receive requests from ALB.\n*AWS Elastic Load Balancer (ELB) service at a glance: * ELB is an AWS load balancing service. It allows distributing requests to instances to ensure performance and reliability. ELBs can help prevent instances from being overloaded by dividing requests evenly, or using health checks to avoid requests to defective instances. In this article, I will use the load balancing type which is Application Load Balancer. This is a load balancing in layer 7, which supports distribution based on the header, protocol of the request.\n2. Auto Scaling Group After the user accesses the ALB\u0026rsquo;s DNS, the ALB will redirect the request to the Auto Scaling Group. The Auto Scaling Group (ASG) is responsible for creating instances to run the app. The created instances will be placed in the private subnet.\n*AWS Auto Scaling Group (ASG) Service Outline: * AWS ASG is a service that allows automatic expansion or narrowing of instances based on presets and instance status. Auto Scaling Group helps keep the system from overloading, cutting costs and increasing availability. Auto Scaling Group will automatically generate additional EC2 istances, increase the number of EC2 instances as traffic increases and reduces the number of EC2 instances as traffic decreases, while replacing health check failed instances. These instances are created and deleted automatically, so an ELB is required to direct traffic to them.\n3. Design network system for app. For safety, the architecture is placed in the VPC, separated into 2 AZ. You can see 2 AZ (us-east-1c and us-east-1d) identical. This is to keep the system running even though the AZ is down. There are 4 subnets in the VPC, 2 public and 2 private. In one of the two public subnets, I have 1 EC2 for the purpose of being a Bastion Host.\nBastion Host* is a better way to protect the system by placing resources in the private subnet, and external access will have to go through the Bastion Host to get inside (SSH access, the rest of the ALB DNS goes through ALB).\nThe remaining 2 private subnets, I set ASG. ASG automatically creates ec2 instances in these two private subnets and directs its request from the outside.\n"
},
{
	"uri": "/2-runtask/2.2-eventands3/",
	"title": "Create a schedule to submit jobs using EventBridge and save the results to S3",
	"tags": [],
	"description": "",
	"content": "After completing the previous step, you already have a complete job setup on the batch. But in order to run a job, you need to submit the job. You can submit a job manually by going to Batch, selecting a job, and Submit a new job. However, if you need to submit jobs every day, on a fixed schedule or follow a certain trigger, manual job submission is not feasible. So we\u0026rsquo;re going to create a schedule to automatically submit jobs. To do this, we\u0026rsquo;ll use Cloudwatch Event and Lambda to create a schedule and submit the job.\nCreate Lambda Go to Lambda and select Create function. It can be left as Default from Scrach because the function is quite simple, select the name, Runtime and Architecture to be x86_64 Select Change default execution role because you need to add roles to the function in order to submit the job. Go to IAM and add the policy awsBatchServiceEventTargetRole for the role Once you\u0026rsquo;ve created your role, you can choose Create function.\nAfter creating the function, paste the following code into the function\n``python\nimport boto3 import os\ndef lambda_handler (event, context):\nclient batch_client = boto3.client (\u0026lsquo;batch\u0026rsquo;)\nload from the environment variables of lambda job_name = os.getenv (‚ÄúJOB_NAME‚Äù) job_queue = os.getenv (‚ÄúJOB_QUEUE‚Äù) job_definition = os.getenv (‚ÄúJOB_DEFINITION‚Äù)\nresponse = batch_client.submit_job ( jobname=Job_name, JobQueue=Job_Queue, Jobdefinition=Job_definition,\n)\nif \u0026lsquo;Jobid\u0026rsquo; in response: return { \u0026lsquo;StatusCode\u0026rsquo;: 200, \u0026lsquo;body\u0026rsquo;: F\u0026quot;Job submitted successfully. Job ID: {response [\u0026lsquo;JoBid\u0026rsquo;]}‚Äù } else: return { \u0026lsquo;StatusCode\u0026rsquo;: 500, \u0026lsquo;body\u0026rsquo;: \u0026lsquo;Failed to submit job to AWS Batch\u0026rsquo; }\nAt the same time, you need to replace the values of **job_name, **job_queue, and **job_definition** with the corresponding values earlier in AWS Batch ## Create an S3 bucket to save results In this workshop, I save the results with boto3 in a bucket named **job-description-process**, inside that bucket, I will create a folder named **indeed-scraper** and save the result. You need to go to the `job_description_analyzer.py` file in the github clone earlier and edit the value of *BUCKET_NAME_RESULT* to your own name. The reason is that S3 is a global service, and the bucket name must be unique and cannot be the case. 1. Go to S3, Create Bucket ![](/images/2023-07-09-18-24-49.png) 1. Select the name of your choice and leave the rest as default ![](/images/2023-07-09-18-26-08.png) 1. Select Create Bucket ## Create Event Bridge 1. Go to Event Bridge and select Schedules ![](/images/2023-07-09-18-31-08.png) 2. Select Create Schedule ![](/images/2023-07-09-18-31-44.png) 3. After naming the schedule, in the Occupancy section select Recurring Schedule, the Schedule Type is Cron-based and fill in the Cron as follows. ``cron 0 6? *** Select the Flexible Time Window to Off and select Next Select AWS Lambda target and select Lambda Function just created and Next For default options on the next page, select Next and finally Create after review\n"
},
{
	"uri": "/6-cdk/6.3-wrapup/",
	"title": "Frontend Webapp Design",
	"tags": [],
	"description": "",
	"content": "In my opinion, this is the hardest part of all three parts when working with CDK. The reason is because this part is very long. Here\u0026rsquo;s an overview of what we\u0026rsquo;re going to do in this section:\nNetwork Infrastructure Design The code below creates a VPC with 2 subnets, 1 public and 1 private. When creating a public subnet with code, AWS routes itself to the Internet Gateway for itself. And when creating a Private Subnet of the type PRIVATE_WITH_NAT, AWS creates its own NAT Gateway.\n``python\nCreate VPC vpc = ec2. Vpc ( scope=self, id=\u0026lsquo;CdkJobScrapingFrontEnd\u0026rsquo;, vpc_name=f\u0026rsquo; {VPC_NAME} \u0026lsquo;,\nset the CIDR for the VPC cidr= \u0026lsquo;10.0.0.0/16\u0026rsquo;,\nenable DNS support enable_dns_support=True,\nsetup 2 subnet subnet_configuration= [\npublic subnet ec2. SubNetConfiguration ( name=\u0026ldquo;CDKjobScrapingFrontendPublic‚Äù, subnet_type=ec2. Subnettype.public, cidr_mask=24, ),\nprivate subnet with NAT ec2. SubNetConfiguration ( name=\u0026ldquo;CDKjobScrapingFrontendPrivate‚Äù, subnet_type=ec2. subnettype.private_with_nat, cidr_mask=24, ), ],\n)\n## Creating Security Groups for EC2 Instances and Bastion Hosts ``python # +++ Create EC2 +++ # create a security group for the ec2 private instance private_ec2_security_group=ec2. SecurityGroup ( scope=self, id=\u0026#34;CDKjobScrapingFrontendSecurityGroup‚Äù, vpc=vpc, allow_all_outbound=True, description=\u0026#34;Security Group for the EC2 private instance‚Äù, security_group_name=\u0026#34;CDK-Jobscraping-frontend-private-SG‚Äù, ) # add the rules to the security group, open custom TPC port 8501 private_ec2_security_group.add_ingress_rule ( peer=ec2. Peer.any_IPv4 (), connection=ec2. Port.TCP (8501), Description=\u0026#34;Allow inbound traffic from anywhere for port 8501\u0026#34;, ) # add ssh port 22 for SG for the EC2 private instance private_ec2_security_group.add_ingress_rule ( peer=ec2. Peer.any_IPv4 (), connection=ec2. Port.TCP (22), description=\u0026#34;Allow inbound traffic from anywhere for port 22\u0026#34;, ) # create security group for bastion host bastion_security_group=ec2. SecurityGroup ( scope=self, id=\u0026#34;CDKjobScrapingFrontendBastionSecurityGroup‚Äù, vpc=vpc, allow_all_outbound=True, description=\u0026#34;Security Group for the bastion host‚Äù, security_group_name=\u0026#34;CDK-Jobscraping-frontend-bastion-SG‚Äù, ) # add the rules to the security group, open SSH port 22 bastion_security_group.add_ingress_rule ( peer=ec2. Peer.any_IPv4 (), connection=ec2. Port.TCP (22), description=\u0026#34;Allow inbound traffic from anywhere for port 22\u0026#34;, ) Create Bastion Host Note: Since I can\u0026rsquo;t find a way to download the pem key when using CDK, you will have to create a pem key and save the name in the environment variable. ``python\ncreate a bastion host AMI for bastion host ami_bastion_host = ec2. Machineimage.generic_Linux ( { ‚Äúus-east-1\u0026rdquo;:\u0026rdquo; ami-053b0d53c279acc90\u0026quot; } )\nDON\u0026rsquo;T FORGET THAT YOU WILL NEED TO CREATE A KEY PAIR IN THE AWS CONSOLE (BECAUSE YOU COULDN\u0026rsquo;T ACCESS IT LATTER IF CREATE HERE) bastion_host = ec2. Instance ( scope=self, id=\u0026ldquo;CDKjobScrapingFrontendBastionHost‚Äù, instance_type=ec2. InstanceType (‚Äút2.micro‚Äù), machine_image=ec2. MachineImage.Latest_Amazon_Linux ( generation=ec2. AmazonLinuxGeneration.Amazon_Linux_2 ), vpc=vpc, vpc_subnets=ec2. SubnetSelection ( subnet_type=ec2. Subnettype.public ), security_group=bastion_security_group, key_name=f\u0026rsquo; {KEY_NAME} \u0026lsquo;, instance_name=\u0026lsquo;cdkjobscrapingfrontendbastionhost\u0026rsquo;, )\n## Create ASG and ALB In the code below, I have: - Create a role for ASG - Create a Launch Instance. You need to assign a pem key if you want to access the instance later. Launch instances also need SG. Also, you need to assign user data to the launch template. - Create ASG - Create ALP - Create Target Group and assign ASG to Target Group ``python # +++ Create Load Balancer and Auto Scaling Group +++ # But first, we will need 2 things #1. Create an AMI that has all the packages installed #3. Create a Role for ASG # Since the first steps would be done in console, I\u0026#39;ll do the 3rd step here # Create a role for ASG Role = iam.Role ( scope=self, id=\u0026#39;CdkJobScrapingFrontEndrole\u0026#39;, assumed_by=Iam.servicePrincipal (\u0026#39;ec2.amazonaws.com\u0026#39;), role_name=\u0026#39;cdkjobscrapingfrontendrole\u0026#39;, managed_policies= [ # Read access to EC2 IAM.ManagedPolicy.From_AWS_Managed_Policy_Name (\u0026#39;Amazonec2ReadOnlyAccess\u0026#39;), # Full access to S3 IAM.managedpolicy.from_aws_managed_policy_name (\u0026#39;AmazonS3fullAccess\u0026#39;), ]] ) # Okay, now let\u0026#39;s move on to create the Load Balancer and Auto Scaling Group # Create Launch Template. Don\u0026#39;t Forget a Key Pair launch_template=ec2. LaunchTemplate ( scope=self, id=\u0026#39;CdkJobScrapingFrontEndLaunchTemplate\u0026#39;, launch_template_name=\u0026#39;cdkjobscrapingfrontendLaunchtemplate\u0026#39;, # set the machine image from the AMI that we created machine_image=ec2. Machineimage.generic_Linux ( { ‚Äúus-east-1\u0026#34;:f\u0026#39; {AMI} \u0026#39; } ), # set the instance type instance_type=ec2. InstanceType (‚Äút2.micro‚Äù), # set the key pair key_name=f\u0026#39; {KEY_NAME} \u0026#39;, # set the security group security_group=private_ec2_security_group, # set the role role=role, # set the block device mapping: use AMI block device mapping user_data=ec2. UserData.custom ( ‚Äú\u0026#39;Content-Type: multipart/mixed; boundary=‚Äù//‚Äù Mime-Version: 1.0 --// Content-Type: text/cloud-config; charset=\u0026#34;us-ascii‚Äù Mime-Version: 1.0 Content-Transfer-Encoding: 7bit Content-Disposition: attachment; filename=‚Äù cloud-config.txt‚Äù #cloud -config cloud_final_modules: - [scripts-user, always] --// Content-Type: text/x-shellscript; charset=\u0026#34;us-ascii‚Äù Mime-Version: 1.0 Content-Transfer-Encoding: 7bit Content-Disposition: attachment; filename=‚Äù userdata.txt‚Äù #! /bin/bash # Run the app.py script cd /home/ubuntu/ sudo streamlit run streamlit-app.py \u0026gt; output.txt 2\u0026gt;\u0026amp;1 --//--\u0026#34;\u0026#39; ), ) # Create Auto Scaling Group auto_scaling_group = autoscaling.autoscalingGroup ( scope=self, vpc=vpc, id=\u0026#39;CdkJobScrapingFrontEndAutoScalingGroup\u0026#39;, # set the name for the Auto Scaling Group auto_scaling_group_name=\u0026#39;cdkjobscrapingfrontendautoscalinggroup\u0026#39;, # set launch template launch_template = launch_template, # VPC private subnet vpc_subnets=ec2. SubnetSelection ( subnet_type=ec2. SubnetType.Private_With_NAT # current we have only 2 private sbunet, so can use this to select all ), # set the min, max, and desired capacity min_capacity=0, max_capacity=2, desired_capacity=1, ) # create a load balancer load_balancer = elbv2. ApplicationLoadBalancer ( scope=self, id=\u0026#39;CdkJobScrapingFrontEndLoadBalancer\u0026#39;, vpc=vpc, Internet_facing=True, load_balancer_name=\u0026#39;cdkjobscrapingfrontendlb\u0026#39;, security_group=private_ec2_security_group, ) # create a target group that targets to the Auto Scaling Group target_group=elbv2. ApplicationTargetGroup ( scope=self, id=\u0026#39;CdkJobScrapingFrontEndTargetGroup\u0026#39;, vpc=vpc, port=8501, protocol=elbv2. ApplicationProtocol.http, targets= [auto_scaling_group], target_group_name=\u0026#39;cdkjobscrapingfrontendtg\u0026#39;, ) # create a listener that listeners to the target group listener = load_balancer.add_listener ( id=\u0026#39;CdkJobScrapingFrontEndListEner\u0026#39;, port=8501, protocol=elbv2. ApplicationProtocol.http, open=True, default_target_groups= [target_group], ) "
},
{
	"uri": "/4-result/",
	"title": "Results",
	"tags": [],
	"description": "",
	"content": "Results Congratulations to you. So you have reached the last step. The rest is to enjoy the results.\nGo back to EC2 and select Load Balancer. Copy DNS name from the newly created Load Balancer It will look like this CDKjobScrapingFrontendlb-1591468448.us-east-1.elb.amazonaws.com Add 8501 port to the rear CDKjobScrapingFrontendlb-1591468448.us-East-1.elb.amazonaws.com:8501\nIf you are on a page like this, congratulations on your success "
},
{
	"uri": "/5-cleanup/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": "General Introduction After completing the workshop. You need to delete resources to avoid additional costs.\nRemove ASG Go back to EC2 and select Auto Scaling Groups Select the ASG you created and select Action -\u0026gt; Delete. Enter the word delete to confirm Remove Load Balancer Go back to EC2, select Load Balancers (on Auto Scaling Groups 1 minute, right on target groups) Select the Load Balancer you created and select Action -\u0026gt; Delete Load Balancer. Type Confirm -\u0026gt; Delete Delete Target Group Go back to EC2 and select Target Groups (on Auto Scaling Groups) Select the Target Group you created and select Action -\u0026gt; Delete. Select Yes, Delete Delete the Instane Go to EC2 and select Instances Select the running instances and select Instance state -\u0026gt; Terminate. Delete AWS Batch Job Definition Go to AWS Batch and select the Job definitions tab. Select the Job Definition you created and select Deregister Delete AWS Batch Job Queue Go to AWS Batch and select the Job Queues tab Select the job queue you created and select Disable After disabling, select the queue again and select Delete Removing the AWS Batch Compute Environment In AWS Batch, select the Compute environments tab Select the Compute Environment you created and select Disable Select Compute Environment again and select Delete (wait a while until the job queue is deleted to delete the compute environment) Delete Codebuild Project Go to AWS Codebuild Select the project you created and select Delete build project Enter delete to confirm, select Delete Delete the AWS ECR Repository Enter AWS ECR Select the repository you created and select Delete. Enter delete to confirm, select Delete Delete VPC Enter VPC Select the VPC you created, Select Action -\u0026gt; Delete VPC Enter delete to confirm, select Delete "
},
{
	"uri": "/6-cdk/6.3-frontend/",
	"title": "Complete, Run CDK and Clean dep",
	"tags": [],
	"description": "",
	"content": "You have completed the bonus section of this workshop. After understanding the basic concepts and operations of each file. You can start creating the system with CDK.\n1. Make sure you are fully prepared Go to the.env file and change it depending on your environment. These are usually resources I can\u0026rsquo;t create with CDK. Make sure you have logged into AWS and go to your github account containing your crawler code 2. Run CDK You can run the following command in the IDE to enter the CDK environment source .venv/bin/activate After entering the CDK environment, you can run the following command to install the required libraries $ pip install -r requirements.txt Then you can synth out Cloudformation files with the following command: cdk synth If bootstrap is required, you can run the following command: cdk bootstrap Finally, deploy with the following command: cdk deploy --all to run all stacks 3. Cleanup After running, if you want to delete all resources, you can run the following command: cdk destroy --all, much faster than manually, right?\n"
},
{
	"uri": "/6-cdk/",
	"title": "Building a workshop with CDK",
	"tags": [],
	"description": "",
	"content": "General Introduction This is the bonus part of this workshop. You will be instructed to rebuild this workshop with CDK.\n*CDK at a glance: * This is an AWS service that allows building and deploying AWS resources using code. CDK supports many languages such as Python, Javascript, Typescript, Java, C#,\u0026hellip; In this workshop, I will use CDK with Python. CDK creates stacks in Cloudformation for resource deployment. So if you get an error or need monitoring, you can go to AWS Cloudformation.\nPreparation First, you need to clone github repo containing CDK code. You can use the following command: ``bash git clone -b cdk https://github.com/MinhThieu145/FCJBootcamp.git 2. After that, you need to set up the AWS CLI to interact with the resource. If you use pip, you can use the following command to install the AWS CLI: `pip install awscli`. To check, you can use the command: `aws --version` 3. After that, you need to sign in to your AWS account. You can use the following command: `aws configure`. This command will ask you to enter the Access Key and Secret Key. You can get these two keys in the IAM section of the AWS Console. 4. Go to AWS IAM, select User and select Add User 5. Go to User Name -\u0026gt; Next 6. In the Permission tab, select Attach policies directly -\u0026gt; Find and select AdministratorAccess -\u0026gt; Next 7. Select Create User 8. Go to IAM -\u0026gt; User and select the user you just created. Select the Security Credential tab. ![](/images/2023-07-20-01-34-39.png) 9. Scroll down to the Access Key section and select Create Access Key 10. Select ‚ÄúUse case‚Äù to Command Line Interface (CLI) -\u0026gt; Tick confirmation -\u0026gt; Next 11. Select Create Access Key 12. Next to the ‚ÄúDone‚Äù button, select Download .csv. This file will contain your Access Key and Secret Key. You can also copy the Access Key and the Secret Access Key directly from the console. ![](/images/2023-07-20-01-38-41.png) 13. Go back to your IDE, open the terminal and run the command `aws configure`. If CLI is installed, there will be a prompt asking for the access key and the Secret access key. Paste in and done. You can also choose the right region for the region you are using ![](/images/2023-07-20-01-41-14.png) 14. So you\u0026#39;re done importing into your AWS account. Now you can run CDK all right now. "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]