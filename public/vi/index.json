[
{
	"uri": "/vi/1-cicd/1.1-setupgithubandecr/",
	"title": "Chuẩn bị Github Repo ",
	"tags": [],
	"description": "",
	"content": "Trong bài này, các bạn sẽ chuẩn bị các files cần thiết cho các bài sau. Mình sẽ đi qua từng phần. Trong bài lab này, mình sẽ sử dụng VSCode để chuẩn bị đầy đủ các file ở môi trường local và đẩy lên Github. Mình cũng sử dùng Console để deploy kiến trúc lên cloud. Ở phần 2 của bài workshop, mình sẽ chỉ cách các bạn có thể deploy lên bằng CDK (Cloud Development Kit) nếu bạn muốn học thêm về IaC.\nChuẩn bị crawler và push lên Github Trước hết, bạn cần chuẩn bị các file crawler và đẩy lên Github. Đây sẽ là crawler mà chúng ta sẽ deploy lên cloud. Bạn hãy vào link Github sau: Github Crawler. Sau khi vào rồi hãy clone repo này về máy của bạn. Bạn có thể clone bằng cách chạy lệnh sau:\ngit clone -b indeed-scraper https://github.com/MinhThieu145/Job-Scraper-Home.git Nếu như bạn bị lỗi ở câu lệnh trên, hãy thử dùng:\ngit --version Nếu như bạn thấy kết quả trả về hiển thị rõ git version. Thì bạn đã cài git thành công. Nếu không, bạn cần cài git trước khi clone repo. Bạn có thể cài git bằng cách tham khảo link sau Cách cài Github cho VsCode. Sau khi clone hoàn tất, bạn sẽ có 1 repo như sau:\n📦Job-Scraper-Home ┣ 📂.git ┣ 📂__pycache__ ┣ 📂result ┣ 📜.gitignore ┣ 📜buildspec.yml ┣ 📜dockerfile ┣ 📜exploration.ipynb ┣ 📜job_description_analyzer.py ┣ 📜main.py ┣ 📜requirements.txt ┗ 📜scraper.py Mình sẽ đi qua từng file và folder mà bạn cần biết trong repo này:\n1. main.py và scraper.py Đầu tiên là file main.py. File có nhiệm vụ chạy file scraper.py và chứa kết quả trong 1 folder tên là result (sẽ tạo folder mới nếu không tồn tại). Sau cùng, script sẽ đẩy tất cả files trong folder result lên S3.\n# import indeed scraper import scraper # import analyze data function import job_description_analyzer # some other libraries import os import pandas as pd # main function def main(): # clean the result folder # get current dir current_dir = os.getcwd() # get the result folder if exit try: # if the result folder exists result_dir = os.path.join(current_dir, \u0026#39;result\u0026#39;) # delete all the files in the result folder for file in os.listdir(result_dir): os.remove(os.path.join(result_dir, file)) except: # create a new folder called result os.mkdir(\u0026#39;result\u0026#39;) result_dir = os.path.join(current_dir, \u0026#39;result\u0026#39;) # run the scraper scraper.main() # get the result from a folder called result # loop through the result folder for file in os.listdir(result_dir): # if that is a csv file if file.endswith(\u0026#39;.csv\u0026#39;): # read with pandas df = pd.read_csv(os.path.join(result_dir, file)) # if the dataframe is not empty if not df.empty: # pass that to the analyze data function job_description_analyzer.main(df=df, df_name=file.split(\u0026#39;.\u0026#39;)[0]) if __name__ == \u0026#34;__main__\u0026#34;: main() Nếu bạn đọc kĩ code, bạn sẽ thấy main.py loop qua tất cả file đuôi .csv trong folder result và gọi 1 function khác trong job_description_analyzer.py trước khi đẩy lên S3. Lý do là vì mình đã cắt ngắn repo đi nhiều. Ban đầu crawler sẽ có nhiệm vụ như sau.\nmain.py sẽ lên S3 để tìm các crawlers (mỗi website là 1 crawler khác nhau) và chạy các crawler này. Các crawler này sau khi chạy xong đều sẽ lưu kết quả vào folder result. - Sau đó, file job_description_analyzer.py có nhiệm vụ extract những từ khoá và những thông tin quan trọng bằng GPT API, trước khi lưu kết quả lại Cuối cùng, cả kết quả scrap ban đầu và sau khi xử lý được đẩy lên S3. Do quá trình này khá lâu, dễ hỏng và không hiệu quả khi gần như xử lý hoàn toàn bằng code. Nếu bạn muốn thử làm lại, bạn có thể sử dụng AWS Step Function và AWS SNS để decouple các phần, nhằm đảm bảo thứ tự thực hiện và độ ổn định của Pipeline. Nhưng trong bài lab này mình sẽ không đi qua phần đó\n2. dockerfile và requirements.txt Đây là 2 files cần thiết để có thể build 1 docker image từ code. Trong File dockerfile, mình đã\nSử dụng based image là Ubuntu:22.04. Nhưng bạn có thể sử dụng những base image khác, miễn là có cài sẵn Python3. Nếu không bạn sẽ phải tự cài đặt Python3 Cài đặt và update pip để tải cài các package cần thiết cho crawler. Tên của các package này được lưu trong file requirements.txt. Trong bài lab này, mình đã sử dụng boto3 để có thể gọi các API của AWS, openai để có thể gọi API của GPT3 (có thể bỏ vì không dùng nữa), pandas để xử lý dữ liệu, selenium để có thể chạy crawler và webdriver_manager để có thể tự động tải driver cho selenium. Tất cả các packages này đều được lưu trong file requirements.txt. Bạn có thể thêm bớt các packages này tùy theo ý muốn. boto3==1.26.165 openai==0.27.7 pandas==1.5.3 selenium==4.10.0 tenacity==8.2.2 webdriver_manager==3.8.6 Cài đặt Chrome browser cho image. Cuối cùng là Copy các file vào docker image Bạn có thể đọc dockerfile của mình ở đây\n# based image: Ubuntubased. BTW, for the PYTHON:3.9 like you used last time. It used the Debianbased image FROM public.ecr.aws/docker/library/ubuntu:22.04 # install a few things RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ bash \\ git \\ curl \\ software-properties-common \\ pip \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* # workdir WORKDIR /srv # okay now pip RUN apt-get -y update RUN pip install --upgrade pip # Copy the requirements.txt file first, for separate dependency resolving and downloading COPY requirements.txt . RUN pip install -r requirements.txt # Install chrome broswer RUN curl -sS -o - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - RUN echo \u0026#34;deb http://dl.google.com/linux/chrome/deb/ stable main\u0026#34; \u0026gt;\u0026gt; /etc/apt/sources.list.d/google-chrome.list RUN apt-get -y update RUN apt-get -y install google-chrome-stable # add the main.py COPY main.py . # add scraper COPY scraper.py . # add job_description_analyzer COPY job_description_analyzer.py . ENTRYPOINT [ \u0026#34;python3\u0026#34; , \u0026#34;main.py\u0026#34; ] 3. buildspec.yml Còn 1 file nữa mà bạn cần chú ý, đó là file buildspec.yml. File này cực kì quan trọng, vì nó hướng dẫn cho AWS Codebuild cách build image của bạn. Buildspec được chia làm 3 phase, pre_build, build và post_build.\nversion: 0.2 phases: pre_build: commands: - echo \u0026#34;Logging in to ECR\u0026#34; - aws --version - AWS_DEFAULT_REGION=us-east-1 - AWS_ACCOUNT_ID=238101178196 - IMAGE_TAG=latest - REPOSITORY_URI=238101178196.dkr.ecr.us-east-1.amazonaws.com/indeed-scraper - aws ecr get-login-password --region $AWS_DEFAULT_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com build: commands: - echo \u0026#34;Building the Docker image...\u0026#34; - docker build -t $REPOSITORY_URI:$IMAGE_TAG . - docker push $REPOSITORY_URI:$IMAGE_TAG post_build: commands: - echo \u0026#34;Updating ECS service...\u0026#34; - echo \u0026#34;End of script\u0026#34; artifacts: files: - \u0026#39;**/*\u0026#39; name: artifacts Trong phase pre_build, mình đã lưu các biến môi trường như sau:\nAWS_DEFAULT_REGION: region mà bạn dùng cho bài workshop này, mình dùng us-east-1 (virginia) AWS_ACCOUNT_ID: Account ID tài khoản AWS của ban, có thể tìm được ở phần account IMAGE_TAG: Tag của image mà bạn muốn lấy trong ECR. Trong quá trình build, bạn sẽ update image nhiều lần, mỗi lần là tag + 1. Nên tag latest giúp bạn lấy image mới nhất REPOSITORY_URI: link của ECR repo chứa image, mình sẽ tạo ngay ở phía dưới. Dòng cuối cùng của phase là để mình authenticate script của mình, giúp mình có thể thực hiện các lệnh trong Docker như Push hay Pull image từ Ecr. Trong phase build, mình sẽ build image và push lên ECR repo.\ndocker build -t $REPOSITORY_URI:$IMAGE_TAG . mình build image và set tag là latest. Đây là biến IMAGE_TAG mình set từ trước. Nhớ đừng quên có 1 dấu chấm sau chữ IMAGE_TAG đấy. docker push $REPOSITORY_URI:$IMAGE_TAG mình push Docker image mình vừa build lên Ecr repo. Ở đoạn này, nếu làm theo mình thì bạn sẽ bị vướn mắc do mình chưa tạo repo, vì vậy bạn có thể để trống biến này, và đọc tiếp phần sau để biết cách tạo repo. Cuối cùng là phase post_build. Mình chỉ log ra console để thông báo hoàn thành. Phase này hoàn toàn không thực hiện lệnh gì cả.\nDeploy Github repo lên Github. Sau khi hoàn thành việc chuẩn bị các file, việc tiếp theo là đẩy repo lên Github. Đầu tiên, bạn cần mở folder mà bạn vừa clone repo về. Trong VSCode sẽ nhìn như thế này Kiểm tra xem bạn đã có git chưa bằng git --version. Tiếp đến, bạn cần remove file .git khỏi folder do folder vẫn liên kết với repo Github của mình. Dùng lệnh này để remove file .git rm -rf .git. Sau đó bạn cần tạo repo mới trên Github của mình. git init -b main để khởi tạo repo mới. git add . để add tất cả các file trong folder vào repo git commit -m \u0026quot;Initial commit\u0026quot; để commit các file vừa add vào repo Lên tài khoản github của bạn, tạo 1 repo mới và copy URL git remote add origin \u0026lt;URL\u0026gt; để add repo mới vào local repo của bạn git push -u origin main để push repo lên Github. Nếu bạn bị lỗi, hãy thử git push -f origin main để force push. Sau khi push xong, bạn có thể thấy repo của bạn đã được update "
},
{
	"uri": "/vi/1-cicd/",
	"title": "Chuẩn bị và thiết kế CI/CD Pipeline",
	"tags": [],
	"description": "",
	"content": "Giới thiệu tổng quát Chào mừng các bạn đến với phần đầu tiên của workshop. Trong phần này, mình sẽ đi qua từng bước để chuẩn bị crawler và đẩy lên Github, thiết kế CodeBuild để có thể tự động build và deploy docker image lên AWS ECR\n"
},
{
	"uri": "/vi/",
	"title": "Giới thiệu và Chuẩn bị",
	"tags": [],
	"description": "",
	"content": "Giới thiệu tổng quát Chào mừng các bạn đến với Workshop 1. Trong bài workshop này, mình sẽ thiết kế kiến trúc cho 1 crawler đơn giản để bạn có thể crawl job postings trên Indeed. Bài lab này sẽ chia làm 3 phần chính:\n1. Thiết kế Pipeline CI/CD Phần 1 của bài lab sẽ là về qui trình CI/CD của crawler. Mình sẽ sử dụng CodeBuild để tự động lấy code từ Githubm từ đó build Docker image cho crawler và tự động đẩy image vào ECR. Phần kiến trúc sẽ thiết kế như sau:\n2. Thiết kế kiến trúc cho crawler Phần 2 của bài lab, cũng là phần chính, sẽ là về việc chạy crawler dưới dạng Docker image bằng Fargate. Mình cũng sẽ cung cấp code cho crawler. Crawler được viết bằng Python và Selenium. Bạn cũng có thể mở rộng để tạo crawler cho các website khác hoặc sửa đổi crawler hiện tại theo ý muốn. Phần kiến trúc bao gồm AWS EventBridge để có thể chạy crawler theo lịch trình, AWS Lambda để gửi job definition vào job queue trong AWS Batch. Sau khi hoàn thành sẽ chứa kết quả vào S3. Kiến trúc thiết kế như sau:\n3. Thiết kế 1 webapp front-end để hiển thị kết quả Phần 3 của bài lab sẽ chú trọng vào việc thiết kế 1 high-availability (sẵn sàng cao) webapp để hiển thị kết quả crawler. Mình sẽ sử dụng Streamlit và viết bằng Python. Webapp sẽ được đặt trong Ec2. Kiến trúc thiết kế như sau: "
},
{
	"uri": "/vi/1-cicd/1.2-setupecrrepo/",
	"title": "Setup repo cho AWS ECR",
	"tags": [],
	"description": "",
	"content": "Chúc mừng bạn đã chuẩn bị xong Github repo của mình. Đây sẽ repo để chứa crawler của bạn và dùng trong CI/CD pipeline sau này. Trong bài này, mình sẽ tạo AWS ECR repo để chứa image sau khi build xong từ Codebuild. Sơ lược về dịch vụ AWS ECR: Đây là 1 dịch vụ của AWS, cho phép quản lý, lưu trữ và triển khai các container. Dịch vụ này được dùng với các dịch vụ chạy Conatiner như AWS ECS, AWS EKS, AWS Fargate, AWS Batch,\u0026hellip; Trong bài workshop này thì mình dùng AWS ECR cùng với AWS Batch\nTạo repo trên AWS ECR Lên AWS, tìm AWS ECR -\u0026gt; chọn Create repository. Các setting bạn để như sau\nVisibility: Chọn chế độ private. Mình từng gặp lỗi không thể pull image được nếu để là public, nên recommend đặt là private Repository name: Đặt tên cho repo, bạn có thể thấy được link repo. Trong phần Repository name, bạn sẽ thấy một chỗ để điền tên. Phần còn lại chính là URI dẫn tới repo của bạn. Mình sẽ đặt tên repo là indeed-scraper, nên link dẫn tới repo của mình sẽ là:\n238101178196.dkr.ecr.us-east-1.amazonaws.com/indeed-scraper Bạn có thể thấy link được tách làm nhiều phần. Trong đó, dãy số đứng đầu 238101178196 là account Id của bạn. Sau chữ ecr là region sử dụng. Mình sử dụng region là us-east-1. Cuối cùng là tên bạn tự đặt, của mình là indeed-scraper\nỞ 3 options còn lại, bạn có thể để là disable Sau khi tạo xong, bạn có thể copy URI của repo vừa tạo và paste vào file buildspec.yml ở bước 2. Đây là phần mình bảo để trống ban nãy do chưa tạo repo "
},
{
	"uri": "/vi/2-runtask/2.1-awsbatch/",
	"title": "Thiết kế AWS Batch",
	"tags": [],
	"description": "",
	"content": "Trong bước này, chúng ta sẽ setup 1 AWS Batch hoàn chỉnh để chạy job. Để sử dụng AWS Batch, bạn cần tạo các components mà mình đã đề cập ở bài trước:\nCompute Environments Job Queues Job Definitions Batch Wizard Để thuận tiện hơn thì nên sử dụng wizard thay vì tạo từng components riêng lẻ. Vào AWS Batch, chọn Wizard. Nếu bạn muốn có nhiều lựa chọn có thể dùng EC2, hoặc EKS nếu dùng Kubernete. Nhưng để thuận tiện, mình sẽ dùng Fargate. Compute Environment Chọn tên cho compute env Spot Instance có thể giúp tiết kiệm chi phí, thế nhưng những task chạy bằng spot instance có thể bị ngắt đoạn giữa chừng, chỉ nên dùng nếu task ngắn hoặc có thể thay thể bởi những job khác. Mình sẽ chọn không trong bài này. Đồng thời chọn vCPU tối đa, có thể để 4 vCPU. Đến bước Network Configuration, bạn có thể setup như sau\nTạo 1 VPC với 1 public subnet Nhớ tạo thêm Internet Gateway và Route table để Subnet truy cập được vào internet Tạo 1 security group cho compute env, mở outbound traffic ra internet (default là mở rồi nên không cần sửa) Thêm các phần đã tạo vào compute env Chọn next để tạo compute env Job Queue Chọn tên cho job queue Đặt Priority là 1. Đây là thứ tự ưu tiên, trong trường hợp bạn có nhiều job queue trong cùng một compute env, job queue có priority cao sẽ được chạy trước. Job Definition Chọn tên và timeout cho job definition. Timeout là thời gian chạy tối đa của 1 job. Nếu job chạy quá thời gian này thì sẽ bị dừng. Mình ước lượng job của mình cần ít hơn 15 phút nên mình để 15 phút (900s). Fargate, bạn để version là LATEST, bật Public IP vì job cần truy cập vào Internet. Ephemeral storage là dung lượng của Fargate, để từ 21GB đến 200GB. Execution role để giúp cho task của bạn có thể truy cập vào ECR để pull image từ đó, đọc Secret từ Secret Manager và 1 vài role low level. trong Container configuration, ở phần image, bạn cần vào ECR, copy URI của ECR repo mà bạn tạo trước đó Paste vào phần Image của Batch Wizard và thêm latest vào phía sau để có thể lấy image mới nhất Phần tiếp theo là command. Bạn có thể thêm command cho Docker Image của bạn. Command này sẽ không overwrite CMD hay ENTRYPOINT trong docker của bạn, mà sẽ thêm vào. Vì mình không cần thêm gì nên sẽ để default, câu lệnh default chỉ để in ra Hello World Có vẻ hơi kì lạ khi có tận 2 role trong phần job definition, tuy nhiên, theo lời giải thích tại đây. Nếu Task của bạn cần quyền high level hơn như truy cập S3(những quyền mà không phải task nào cũng cần), thì bạn có thể để ở job role, còn những quyền thấp hơn như pull image từ ECR thì có thể cố định ở execution role. Task của mình cần truy cập vào S3 nên sẽ tạo 1 role Full Access S3, tuy nhiên nếu không cần gì ngoài chạy task thì bạn có thể đê trống (chọn None). Đồng thời bạn cần chọn vCPU và Memory Chọn tên cho Job của bạn. Khi mà chạy thì tên này sẽ được hiển thị. Sau khi hoàn thành, chọn Review và Create Resource. Vậy là bạn đã hoàn thành việc setup AWS Batch. Để kiểm tra, bạn có thể vào Job Definition và chọn Submit new Job\n"
},
{
	"uri": "/vi/3-frontendapp/3.1-vpc/",
	"title": "Thiết kế môi trường mạng",
	"tags": [],
	"description": "",
	"content": "Bước đầu tiên chính là thiết kế môi trường mạng cho app. Như đã để cập trước đó, đây là những tài nguyên mình sẽ tạo:\n2 public subnet, 2 private subnet 1 VPC 1 Internet Gateway 1 NAT Gateway (có thể tạo 2 NAT Gateway nếu muốn an toàn hơn - mỗi NAT ở 1 subnet) S3 Endpoint để truy cập S3 mà không cần đi qua NAT Gateway Setup VPC và subnet Chọn VPC, Create VPC. Chọn vào option VPC and more. Đặt tên cho VPC. Bạn có thể để defaul CIDR: 10.0.0.0/16, bạn cũng có thể thay CIDR để tăng giảm số lượng IP trong VPC. Bạn có thể để IPV6 như default (không dùng IPV6) Number of Availability Zones: số AZ mà VPC sẽ được tạo. Theo diagram, mình sẽ để 2 AZ. Bạn cũng có thể chọn Customize AZs để chọn sang những AZ khác (default là us-east-1a và us-east-1b nếu dùng us-east-1) Number of public subnets: bao nhiêu public subnet trong VPC. Mình cần 2 public subnet (mỗi AZ 1 public subnet) nên sẽ để 2. Đồ thị ở bên phải cũng chỉ rõ ra từng phần của public subnet: mỗi AZ có 1 subnet, subnet được thông qua Route table để nối đến Internet Gateway. Number of private subnets: bao nhiêu private subnet trong VPC. Mình cũng cần 2 private (1 AZ 1 private). Nếu chọn 4, bạn có thể thấy số lượng private subnet tăng lên 1 ở mỗi AZ Customize subnets CIDR blocks: Bạn có thể điều chỉnh thông số này để tăng giảm số địa chỉ IP mỗi subnet. Hiện tại, mỗi submet có 4096 IP NAT gateways ($) Đây là phần khá tốn kém. Nếu muốn an toàn và đúng theo diagram, bạn có thể chọn 1 per AZ để mỗi AZ có 1 NAT. Bạn cũng có thể chọn In 1 AZ để chỉ tạo 1 NAT gateway cho cả 2 private subnet.\nVPC endpoints Nên chọn để giảm chi phí truy cập S3. Do nếu bạn dùng boto3 mà không có Endpoint thì phải buộc đi qua NAT (do dữ liệu truyền ra internet). Việc có Endpoint sẽ giảm chi phí này NAT (nếu thực sự không cần Internet) và phí data transfer.\nDNS option chọn cả 2 options\nChọn Create VPC để hoàn thành.\n"
},
{
	"uri": "/vi/1-cicd/1.3-deployoncloud/",
	"title": "Deploy CI/CD pipeline lên AWS",
	"tags": [],
	"description": "",
	"content": "Sau khi hoàn thành các bước chuẩn bị, thì bạn cũng đã có đầy đủ các file cần thiết để deploy CI/CD pipeline lên AWS. Trong bài này, mình sẽ hướng dẫn bạn cách deploy CI/CD pipeline lên AWS bằng cách sử dụng AWS Codebuild\nSơ lược về dịch vụ AWS Codebuild: AWS Codebuild cho phép tự động build, test và containerize code của bạn. Codebuild có thể dùng với AWS CodePipeline để tự động deploy sau khi build. Tuy nhiên trong bài này mình chỉ build và push image lên ECR thôi.\nTạo Codebuild Project Tìm Codebuild trong AWS, chọn Create build project\nTrong phần Project configuration, điền tên project và Description Ở phần Source, chọn Github là provider Ở phần Repository, chọn Connect to Github và chọn repo mà bạn đã tạo ở bước trước. Bạn có thể bị yêu cầu connect với Github ở bước này, nhập password vào kết nối thôi. Tiếp đến phần Primary source webhook events, tick options webhook để build bằng webhook, chọn single build và event type là PUSH Trong phần Environment, chọn Managed image (Docker image đã bao gồm Operating System rồi, nên phần này thật tế không cần thiết. Nhưng nếu bạn không dùng Docker và cần 1 vài packages thì có thể dùng Custom Image). Operating System chọn Ubuntu, Runtime là Standard và version chọn latest 7.0. Environment type chọn Linux không có GPU.\nBạn đặt biệt phải chọn Privileged ở phần Additional configuration. Nếu không chọn thì sẽ không thể push image lên ECR được.\nChọn New Role Ở phần Buildspec, chọn Use a buildspec file, và để trống ô name, vì chỉ cần đặt lại tên nếu không để file ở root folder hoặc dùng tên khác Những phần còn lại để nguyên và chọn Create build project. Vậy là bạn đã hoàn tất các bước để có 1 pipeline tự động lấy code từ Github, buil docker image và push lên ECR repo.\n"
},
{
	"uri": "/vi/3-frontendapp/3.2-elb/",
	"title": "Tạo tài nguyên: ALB và ASG",
	"tags": [],
	"description": "",
	"content": "Sau khi hoàn tất quá trình tạo VPC, bạn cần tạo thêm các tài nguyên trong VPC. Trong bài này, mình sẽ tạo những tài nguyên sau:\nLaunch Template: Đây giống như 1 bản vẽ. Những instace được tạo ra từ ASG sẽ giống với bản vẽ này. Và để tạo ra Launch Template với đầy đủ các packages cần thiết, mình sẽ tạo sẵn một AMI Sơ lược về Amazon Machine Image (AMI): Có thể hiểu AMI là bản vẽ của instance. Khi tạo 1 EC2 instance mới, bạn được hỏi chọn AMI. Đa phần bạn sẽ dùng base image, những AMI được cung cấp bởi AWS như là ubuntu, linux hay window. Tuy nhiên, nếu muốn EC2 được tạo với các package có sẵn, ta có thể tạo custom image.\nASG: Tạo ASG bằng Launch Template phía trên ALB: Tạo ALB để điều hướng request Tạo Custom AMI và Launch Template Khi các Instance được khởi tạo từ ASG, chúng cần 1 khuông mẫu (template). Để tạo ra 1 launch template, trước tiên ta cần tạo 1 custom AMI với tất cả package cần thiết.\nVào Ec2, chọn Launch Template rồi chọn Create launch template Đặt tên cho template rồi chọn option Auto Scaling guidance để thuận tiện tích hợp với ASG sau này. Chọn AMI, bấm vào Browse more AMIs Nhập số 238101178196 vào thanh tìm kiếm rồi chọn Community AMI, bạn nên thấy 1 AMI với tên như hình dưới. Đây là AMI mình public, chữa sẵn các file chạy server. Nếu thực hiện đúng như các bước trước, đặt biệt là cách đặt tên bucket của S3, bạn có thể sử dụng mà không cần thay đổi gì. Bên trong AMI cài sẵn 1 streamlit server với cổng 8501, đồng thời lưu kết quả vào S3 trong Bucket tên là job-description-process, và lấy từ folder tên indeed-scraper\nChọn select AMI đó, bạn sẽ được đưa lại chỗ template để tiếp tục. Ở phần Instance Type chọn t2.micro. Ở phần Key pair chọn 1 key pair mà bạn đã có sẵn để sau này thuận tiện SSH vào instance (tất nhiên là phải qua Bastion Host) Phần Network Settings bạn có thể để trống (không chọn gì cả) Bạn có thể để những phần còn lại default.\nỞ Advanced details, expand ra và chọn vào Create new IAM profile, do Frontend app cần 1 vài roles (Truy cập S3 và đọc Ec2). Tạo 1 role mới có các policies như sau Sau khi tạo xong, quay lại template, reload (cái mũi tên tròn bên cạnh) để thấy role vừa tạo và chọn Sau đó chọn Create launch template để tạo template.\nTạo Auto Scaling Group Vào Ec2, kéo xuống cuối cùng để thấy được ALB. Chọn Create Auto Scaling Group và chọn tên cho ASG. Sau đó trong phần Launch Template, chọn Template mà bạn tạo ban nãy, chọn version là Latest Ở tab Network, chọn VPC mà bạn tạo ban nãy. Trong VPC đó, chọn cả hai Private Subnet mà bạn tạo ban nãy. Ta sẽ đặt ứng dụng ở Private Subnet để tăng tính bảo mật, Public Subnet chỉ để tạo NAT Gateway và đặt Bastion Host. Bấm next Ở tab Load Balancing, chọn Attach to new load balancer . Bên trong tab mới mở, chọn Application Load Balancer, chọn tên cho Load Balancer và chọn Internet-facing Kế đến là phần Network mapping, VPC sẽ được giữ nguyên như của ASG (tất nhiên, ELB để handle traffic vào ASG nên cần phải chung VPC rồi). Bạn cần chọn 2 Public Subnet để đặt ELB Ở phần Listener và Routing, chọn Port là 8501 cho Streamlit, và chọn Create a target Group và đặt tên cho target group Trong phần Health checks, chọn option Turn on Elastic Load Balancing health checks. Đồng thời đặt Health check grace period là 300s. Đây là thời gian ước lượng để Ec2 khởi động (đây là cách để Ec2 instance tránh bị destroy khi chưa khởi động xong. Nếu User Data khiến instance mất thời gian khởi động, hãy ước lượng thời gian cần thiết để khởi động ở đây). Để phần Additional settings như default và bấm Next\nGroupsize bao gồm 3 phần: Maximum capacity để chỉ số instance tối đa, Minimum capacity là số instance tối thiểu, Desired capacity là số instance mà ASG cố giữ ở mức đó. Để trống phần Scaling policies và chọn Next\nBỏ qua phần Add notifications và chọn Next. Bỏ qua tags và next rồi chọn Create Auto Scaling Group\n"
},
{
	"uri": "/vi/2-runtask/",
	"title": "Thiết kế AWS Batch và S3 để chạy crawler và lưu kết quả",
	"tags": [],
	"description": "",
	"content": "Giới thiệu tổng quan Chúc mừng các bạn đã hoàn thành phần 1 của workshop. Trong phần này, mình sẽ thiết kế AWS Batch để có thể tự động lấy các image từ ECR repo mà mình đã làm ở phần trước và chạy theo lịch trình có sẵn. Mình cũng sẽ tạo S3 bucket để lưu kết quả. Những dịch vụ mà mình sẽ sử dụng trong bài này:\nAWS Batch: Để chạy task crawler của mình AWS S3: Để lưu trữ kết quả AWS EventBridge: Để chạy task theo lịch trình. Do nếu chỉ dùng AWS Batch, bạn sẽ phải submit job thủ công. Nên mình sẽ dùng EventBridge để tự động submit job vào Batch theo lịch trình có sẵn AWS Lambda: Tuy EventBridge có thể đặt lịch trình. Tuy nhiên, để có thêm nhiều tính năng, mình sẽ dùng Lambda để submit job vào Batch. Lambda sẽ được trigger bởi EventBridge một cách vô cùng dễ dàng. Và tự Lambda, mình sẽ dùng boto3 để kết nối với Batch và submit job. Giới thiệu về AWS Batch AWS Batch là 1 dịch vụ serverless, cho phép bạn chạy các Batch job. AWS Batch sẽ tự động scale các node nhỏ hơn. Giúp bạn tiết kiệm chi phí và chạy job nhanh hơn. Mình sẽ đi qua từng phần của AWS Batch:\nCompute Environment: Đây là nơi để bạn chọn compute resource phù hợp với nhu cầu của bạn. Do Batch là dịch vụ serverless nên bạn sẽ không phải setup quá nhiều Job queue: 1 queue để chứa các job. Khi bạn submit job vào Batch, các job sẽ nằm trong queue Job definition: là định nghĩa của job. Bạn có thể tưởng tượng đây là bản vẽ của job mà bạn muốn thực hiện. Khi muốn thực hiện job definition này, thì bạn submit nó vào queue. Giới thiệu về S3 Đây là dịch vụ khá nổi tiếng của AWS. S3 là dịch vụ lưu trữ dạng object. S3 có thể lưu các định dạng file khác nhau, code, video hay hình ảnh,\u0026hellip;\n"
},
{
	"uri": "/vi/6-cdk/6.1-cicd/",
	"title": "Thiết kế CI/CD Pipeline",
	"tags": [],
	"description": "",
	"content": "Trong phần lab trước, bạn đã học cách tạo pipeline cho crawler với Codebuild. Trong phần này, mình sẽ sử dụng CDK để thực hiện điều tương tự. Trong repo mà bạn clone về, trong folder workshop1 có 1 file python là: cicd_stack.py. Đây là file setup CI/CD pipeline cho crawler. Mình sẽ đi qua từng phần trong file này. Đầu tiên là tổng quan file:\nfrom aws_cdk import ( # Duration, Stack, # aws codebuild aws_codebuild as codebuild, # aws iam aws_iam as iam, ) from constructs import Construct # Some pther lib import os # load the data for the CodeBuild from environment variables from dotenv import load_dotenv load_dotenv() GITHUB_OWNER = os.getenv(\u0026#34;OWNER\u0026#34;) GITHUB_REPO = os.getenv(\u0026#34;REPO\u0026#34;) GITHUB_BRANCH = os.getenv(\u0026#34;BRANCH\u0026#34;) # please use this line to add credential to the CodeBuild # aws codebuild import-source-credentials --server-type GITHUB --auth-type PERSONAL_ACCESS_TOKEN --token \u0026lt;token_value\u0026gt; # The personal Token need to have enough permission to access the repo class CICDStack(Stack): def __init__(self, scope: Construct, construct_id: str, **kwargs) -\u0026gt; None: super().__init__(scope, construct_id, **kwargs) # Create the CodeBuild Project # Create Github Source github_source = codebuild.Source.git_hub( owner=f\u0026#34;{GITHUB_OWNER}\u0026#34;, repo=f\u0026#34;{GITHUB_REPO}\u0026#34;, webhook=True, branch_or_ref=f\u0026#34;{GITHUB_BRANCH}\u0026#34;, # there are an option called webhook_filters: that is for the Webhook Event Filter on AWS. Leave it as default ) # Create the role for the CodeBuild Project codebuild_role = iam.Role( scope=self, id=\u0026#34;JobScrapingCICDRole\u0026#34;, role_name=\u0026#34;JobScrapingCICDRole\u0026#34;, assumed_by=iam.ServicePrincipal(\u0026#34;codebuild.amazonaws.com\u0026#34;), # add the full S3 access to the role managed_policies=[ iam.ManagedPolicy.from_aws_managed_policy_name( managed_policy_name=\u0026#34;AmazonS3FullAccess\u0026#34; ), # add the full ECR access to the role iam.ManagedPolicy.from_aws_managed_policy_name( managed_policy_name=\u0026#34;AmazonEC2ContainerRegistryFullAccess\u0026#34; ), ] ) # Create the CodeBuild Project codebuild.Project( scope=self, id=\u0026#34;CICDProject\u0026#34;, project_name=\u0026#34;CICDProject\u0026#34;, # add description description=\u0026#34;CodeBuild Project for the CICD for Job Scraping Project\u0026#34;, # set the source source=github_source, # add some environment variables environment=codebuild.BuildEnvironment( # add the Base Environment Variable build_image=codebuild.LinuxBuildImage.STANDARD_7_0, # set the priviledge to true for docker build privileged=True, ), # Create role for the CodeBuild Project role=codebuild_role, ) Chuẩn bị các biến môi trường cho Codebuild. Thay vì connect trực tiếp vào Github như khi sử dụng console, bạn buộc phải sử dụng environment variable để truyền thông tin vào Codebuild. Mình sẽ sử dụng thư viện dotenv để load các biến môi trường từ file .env, nhưng bạn có thể để trong code.\n# load the data for the CodeBuild from environment variables from dotenv import load_dotenv load_dotenv() GITHUB_OWNER = os.getenv(\u0026#34;OWNER\u0026#34;) GITHUB_REPO = os.getenv(\u0026#34;REPO\u0026#34;) GITHUB_BRANCH = os.getenv(\u0026#34;BRANCH\u0026#34;) Bên cạnh đó, mình cũng import các thư viện cần thiết cho stack\nfrom aws_cdk import ( # Duration, Stack, # aws codebuild aws_codebuild as codebuild, # aws iam aws_iam as iam, ) from constructs import Construct # Some pther lib import os # load the data for the CodeBuild from environment variables from dotenv import load_dotenv load_dotenv() Còn một bước rất quan trọng để bạn chạy được Gitbuild, đó là cấp quyền access Github cho AWS. Bạn cần lấy personal access key từ Github và sử dụng lệnh sau.\naws codebuild import-source-credentials --server-type GITHUB --auth-type PERSONAL_ACCESS_TOKEN --token \u0026lt;token_value\u0026gt; Khi tạo personal access từ Github, bạn cần cấp đủ quyền cho AWS. Nếu bị lỗi thì cần kiểm tra lại quyền của token\nTạo Github Source Ở phần console thì bạn chọn Source từ console như trong ảnh: Nhưng trong CDK, bạn cần tạo Github Source bằng code. Bạn có thể tham khảo thêm tại Github Source.\n# Create Github Source github_source = codebuild.Source.git_hub( owner=f\u0026#34;{GITHUB_OWNER}\u0026#34;, repo=f\u0026#34;{GITHUB_REPO}\u0026#34;, webhook=True, branch_or_ref=f\u0026#34;{GITHUB_BRANCH}\u0026#34;, # there are an option called webhook_filters: that is for the Webhook Event Filter on AWS. Leave it as default ) Tạo Environment Trong console, ở tab Environment, bạn cần\nChọn Operating System (mình dùng Ubuntu) tick priviledge để cho phép docker build Tạo 1 role mới hay chọn role có sẵn Trong CDK, để tạo 1 role mới\n# Create the role for the CodeBuild Project codebuild_role = iam.Role( scope=self, id=\u0026#34;JobScrapingCICDRole\u0026#34;, role_name=\u0026#34;JobScrapingCICDRole\u0026#34;, assumed_by=iam.ServicePrincipal(\u0026#34;codebuild.amazonaws.com\u0026#34;), # add the full S3 access to the role managed_policies=[ iam.ManagedPolicy.from_aws_managed_policy_name( managed_policy_name=\u0026#34;AmazonS3FullAccess\u0026#34; ), # add the full ECR access to the role iam.ManagedPolicy.from_aws_managed_policy_name( managed_policy_name=\u0026#34;AmazonEC2ContainerRegistryFullAccess\u0026#34; ), ] ) Sau đó, bạn có thể tạo Codebuild Project. Các phần như operating system, tick priviledge đều có thể làm ở đây. Bạn có thể thấy trong đoạn code bên dưới, mình đã đặt tên, description, gán source là github, tạo environment là Linux Standard 7.0 giống với console, tick priviledge và chọn role là role vừa tạo.\n# Create the CodeBuild Project codebuild.Project( scope=self, id=\u0026#34;CICDProject\u0026#34;, project_name=\u0026#34;CICDProject\u0026#34;, # add description description=\u0026#34;CodeBuild Project for the CICD for Job Scraping Project\u0026#34;, # set the source source=github_source, # add some environment variables environment=codebuild.BuildEnvironment( # add the Base Environment Variable build_image=codebuild.LinuxBuildImage.STANDARD_7_0, # set the priviledge to true for docker build privileged=True, ), # Create role for the CodeBuild Project role=codebuild_role, ) Vậy là bạn đã hoàn tất công đoạn CI/CD\n"
},
{
	"uri": "/vi/2-runtask/2.2-eventands3/",
	"title": "Tạo lịch trình để submit job bằng EventBridge và lưu kết quả vào S3",
	"tags": [],
	"description": "",
	"content": "Sau khi hoàn thành bước trước, bạn đã có 1 job setup hoàn chỉnh trên Batch. Nhưng để chạy job, bạn cần submit job. Bạn có thể submit job thủ công bằng cách vào Batch, chọn job và Submit new Job Tuy nhiên, nếu bạn cần submit job mỗi ngày, hay theo một lịch trình cố định hay theo một trigger nào đó thì việc submit job thủ công là không khả thi. Vì vậy, chúng ta sẽ tạo 1 schedule để tự động submit job. Để làm được điều này, chúng ta sẽ sử dụng Cloudwatch Event và Lambda để tạo 1 schedule và submit job.\nTạo Lambda Vào Lambda, chọn Create function Có thể để là Default from Scrach do function khá đơn giản, chọn tên, Runtime và Architecture để là x86_64 Chọn Change default execution role do bạn cần thêm role cho function để có thể submit job. Vào IAM vào thêm policy AWSBatchServiceEventTargetRole cho role Sau khi tạo xong role, có thể chọn Create function\nSau khi tạo xong function, paste đoạn code sau vào function\nimport boto3 import os def lambda_handler(event, context): # client batch_client = boto3.client(\u0026#39;batch\u0026#39;) # load from the environment variables of lambda job_name = os.getenv(\u0026#34;JOB_NAME\u0026#34;) job_queue = os.getenv(\u0026#34;JOB_QUEUE\u0026#34;) job_definition = os.getenv(\u0026#34;JOB_DEFINITION\u0026#34;) response = batch_client.submit_job( jobName=job_name, jobQueue=job_queue, jobDefinition=job_definition, ) if \u0026#39;jobId\u0026#39; in response: return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: f\u0026#34;Job submitted successfully. Job ID: {response[\u0026#39;jobId\u0026#39;]}\u0026#34; } else: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;body\u0026#39;: \u0026#39;Failed to submit job to AWS Batch\u0026#39; } Đồng thời, bạn cần thay các giá trị của job_name, job_queue và job_definition bằng giá trị tương ứng ban nãy trong AWS Batch\nTạo S3 bucket để lưu kết quả Trong workshop này, mình lưu kết quả bằng boto3 vào 1 bucket tên là job-description-process, bên trong bucket đó, mình sẽ tạo 1 folder tên là indeed-scraper và lưu kết quả. Bạn cần vào file job_description_analyzer.py ở github clone ban nãy và sửa giá trị của BUCKET_NAME_RESULT thành tên của riêng bạn. Lý do là vì S3 là dịch vụ global, và tên bucket phải là duy nhất, không thể trùng.\nVào S3, Create bucket Chọn tên tuỳ ý và để phàn còn lại là default Chọn Create bucket\nTạo Event Bridge Vào Event Bridge, chọn Schedules Chọn Create Schedule Sau khi đặt tên cho schedule, ở phần Occurence chọn Recurring Schedule, Schedule Type là Cron-based và điền Cron như sau\n0 6 ? * * * Chọn Flexible time window là Off và chọn Next Chọn target là AWS Lambda và chọn Lambda Function vừa tạo và Next Để default options ở trang tiếp theo, chọn next và cuối cùng là Create sau khi review\n"
},
{
	"uri": "/vi/3-frontendapp/",
	"title": "Tạo Webapp Frontend để xem kết quả",
	"tags": [],
	"description": "",
	"content": "Giới thiệu tổng quan Nếu bạn đã đến được đây, chúc mừng bạn đã gần như hoàn thành workshop. Ở các phần trước, Crawler đã có thể chạy theo lịch trình và lưu kết quả vào S3. Tuy nhiên, vẫn chưa có cách nào để bạn có thể coi những kết quả ấy một cách thuận tiện, hay chia sẻ những kết quả ấy với người khác. Trong bài này, mình sẽ tiếp tục thiết kế một webapp bằng streamlit để hiển thị kết quả từ S3.\nGiới thiệu về Streamlit Streamlit là 1 open-source library, dùng python để xây dựng các webapp. Streamlit chủ yếu được dùng để chia sẻ các kết quả của data science. Tuy nhiên, bạn có thể dùng nó để xây dựng các webapp cho mục đích khác. Streamlit có thể chạy trên local hoặc trên cloud. Trong bài này, mình sẽ chạy trên Ec2.\nThiết kế kiến trúc Do kiến trúc phần này hơi dài nên mình sẽ đi rất kĩ. Trước hết bạn hãy đọc thật kĩ diagram architecture của phần này. Mình sẽ đi theo thứ tự mà client sẽ đi (tức là đi từ phía client tới phía server). 1. Application Load Balancer Đầu tiên, user sẽ truy cập vào DNS của Application Load Balancer (ALB) thông qua trình duyệt. ALB sẽ chuyển hướng request đến các Auto Scaling Group, từ đó truy cập vào app được đặt trong private subnet. Bản thân ALB phải được đặt trong public subnet, vì nó sẽ nhận request từ internet. Còn app sẽ được đặt trong private subnet vì nó sẽ nhận request từ ALB.\nSơ lược về dịch vụ AWS Elastic Load Balancer (ELB): ELB là dịch vụ cân bằng tải của AWS. Nó cho phép phân phối các request lên các instance để đảm bảm hiệu năng và độ tin cậy. ELB có thể giúp các instance không bị quá tải bằng cách phân chia đều request, hay sử dụng health check để tránh request đến các instance bị lỗi. Trong bài này, mình sẽ sử dụng loại cân bằng tải là Application Load Balancer. Đây là cân bằng tải ở layer 7, hỗ trợ phân phối dựa vào header, protocol của request.\n2. Auto Scaling Group Sau khi user truy cập đến DNS của ALB, ALB sẽ chuyển hướng request đến các Auto Scaling Group. Auto Scaling Group (ASG) sẽ chịu trách nhiệm tạo ra các instance để chạy app. Các instance được tạo ra sẽ được đặt trong private subnet.\nSơ lược về dịch vụ AWS Auto Scaling Group (ASG): AWS ASG là dịch vụ cho phép tự động mở rộng hoặc thu hẹp instance dựa trên điều kiện đặt trước và tình trạng của instance. Auto Scaling Group giúp hệ thống không bị qúa tải, cắt giảm chi phí và tăng availability. Auto Scaling Group sẽ tự động tạo thêm các EC2 istance, tăng số lượng EC2 instance khi traffic tăng và giảm số lượng EC2 instance khi traffic giảm, đồng thời thay thế các instance bị lỗi (health check failed). Các Instance này được tạo mới và xoá bỏ 1 cách tự động, nên cần có ELB để điều hướng traffic vào chúng.\n3. Thiết kế hệ thống mạng cho app. Để đảm bảo an toàn, kiến trúc được đặt trong VPC, tách làm 2 AZ. Bạn có thể thấy 2 AZ (us-east-1c và us-east-1d) giống hệt nhau. Đây là để cho hệ thống vẫn hoạt động dù AZ bị sập. Có tổng cộng 4 subnet trong VPC, 2 public và 2 private. Ở 1 trong 2 public subnet, mình đã để 1 Ec2 với mục đích làm Bastion Host.\nBastion Host là cách bảo vệ hệ thống tốt hơn bằng việc đặt resource trong private subnet, và truy cập từ ngoài sẽ phải đi qua Bastion Host để vào đến bên trong (truy cập SSH, còn lại nếu dùng DNS của ALB thì đi qua ALB).\n2 Private subnet còn lại, mình đặt ASG. ASG sẽ tự động tạo các Ec2 instance trong 2 private subnet này và điều hướng request của mình từ ngoài vào đó.\n"
},
{
	"uri": "/vi/6-cdk/6.2-runtask/",
	"title": "Thiết kế AWS Batch",
	"tags": [],
	"description": "",
	"content": "Trong phần trước, sau khi thiết kế xong CI/CD pipeline thì mình đã tiếp tục với AWS Batch. Trong phần này, mình sẽ đi qua từng bước để thiết kế AWS Batch bằng CDK, bao gồm\nTạo role cho Batch. Trong AWS Batch console có 1 khung là Execution role. Mình cần tạo và gán role này cho Batch Thiết kế hạ tầng mạng VPC cho Batch Tạo Security Group cho Batch Tạo các components còn lại cho Batch: Compute Env, Job Definition, Job Queue,\u0026hellip; Tạo EventBridge để có thể Schedule cho Batch chạy theo lịch trình Nếu bạn thắc mắc tại sao không có Bucket. Thì trong workshop này mình đã tạo bucket thủ công. Lý do là vì việc pass tên bucket vào script crawler khá khó khăn do cách mình viết script. Do đó mình sẽ phải tạo 1 Bucket, pass tên bucket vào script crawler, sau đó mới có thể chạy crawler.\nfrom aws_cdk import ( # Duration, Stack, # iam aws_iam as iam, # aws batch aws_batch as batch, # lambda aws_lambda as aws_lambda, # Duration Duration, # aws events aws_events as events, # aws events targets aws_events_targets as targets, # aws ec2 (for vpc) aws_ec2 as ec2, ) from constructs import Construct # Other lib import os # load the environment variables from dotenv import load_dotenv load_dotenv() # load ECR_REPO_NAME = os.getenv(\u0026#34;ECR_REPO\u0026#34;) JOB_DEFINITION_NAME = os.getenv(\u0026#34;JOB_DEFINITION\u0026#34;) JOB_QUEUE_NAME = os.getenv(\u0026#34;JOB_QUEUE\u0026#34;) JOB_NAME = os.getenv(\u0026#34;JOB_NAME\u0026#34;) JOB_ROLE_NAME = os.getenv(\u0026#34;ECS_ROLE_NAME\u0026#34;) BATCH_VPC_NAME = os.getenv(\u0026#34;BATCH_VPC_NAME\u0026#34;) BATCH_SUBNET_NAME = os.getenv(\u0026#34;BATCH_SUBNET_NAME\u0026#34;) BATCH_SG_NAME = os.getenv(\u0026#34;BATCH_SG_NAME\u0026#34;) class RunTaskStack(Stack): def __init__(self, scope: Construct, construct_id: str, **kwargs) -\u0026gt; None: super().__init__(scope, construct_id, **kwargs) # aws batch # For the simplicity of this, let create a role for ECS job ecs_role = iam.Role( scope=self, id=f\u0026#34;{JOB_ROLE_NAME}\u0026#34;, assumed_by=iam.ServicePrincipal(\u0026#39;ecs-tasks.amazonaws.com\u0026#39;), role_name=f\u0026#39;{JOB_ROLE_NAME}\u0026#39;, managed_policies=[ iam.ManagedPolicy.from_aws_managed_policy_name(\u0026#39;service-role/AmazonECSTaskExecutionRolePolicy\u0026#39;), # add full access to S3 iam.ManagedPolicy.from_aws_managed_policy_name(\u0026#39;AmazonS3FullAccess\u0026#39;), # add full access to ECR iam.ManagedPolicy.from_aws_managed_policy_name(\u0026#39;AmazonEC2ContainerRegistryFullAccess\u0026#39;), ], ) # +++ Create a VPC for the Batch +++ vpc = ec2.Vpc( scope=self, id = f\u0026#34;{BATCH_VPC_NAME}\u0026#34;, vpc_name=f\u0026#34;{BATCH_VPC_NAME}\u0026#34;, # set the max azs to 1 max_azs=1, # set the cidr cidr=\u0026#39;10.0.0.0/16\u0026#39;, # set the subnet configuration subnet_configuration=[ ec2.SubnetConfiguration( name=f\u0026#34;{BATCH_SUBNET_NAME}\u0026#34;, subnet_type=ec2.SubnetType.PUBLIC, cidr_mask=24, ), ], ) # Create a Security Group for the Batch in the VPC batch_security_group = ec2.SecurityGroup( scope=self, id=f\u0026#34;BatchSecurityGroup\u0026#34;, vpc=vpc, security_group_name=f\u0026#34;{BATCH_SG_NAME}\u0026#34;, # allow all outbound traffic allow_all_outbound=True, ) # create compute environment compute_environment = batch.CfnComputeEnvironment( scope=self, id=\u0026#34;JobScrapingComputeEnvironment\u0026#34;, compute_environment_name=\u0026#39;job-scraping-compute-environment\u0026#39;, # if you set the type as MANAGED, it will pass in the the settings from Launch Template, otherwise you need to set the settings manually type=\u0026#39;MANAGED\u0026#39;, state=\u0026#39;ENABLED\u0026#39;, # Compute Resources: to set the type of compute resource compute_resources=batch.CfnComputeEnvironment.ComputeResourcesProperty( maxv_cpus=4, subnets=[vpc.public_subnets[0].subnet_id], type=\u0026#39;FARGATE_SPOT\u0026#39;, # Security Group security_group_ids=[batch_security_group.security_group_id], ), # Service Role Not neccessary # service_role=batch_role.role_arn, ) # create job queue job_queue = batch.CfnJobQueue( scope=self, id=f\u0026#34;{JOB_QUEUE_NAME}\u0026#34;, job_queue_name=f\u0026#39;{JOB_QUEUE_NAME}\u0026#39;, priority=1, state=\u0026#39;ENABLED\u0026#39;, compute_environment_order=[ batch.CfnJobQueue.ComputeEnvironmentOrderProperty( compute_environment=compute_environment.ref, order=1, ) ], ) # create job definition job_definition = batch.CfnJobDefinition( scope=self, id=f\u0026#34;{JOB_DEFINITION_NAME}\u0026#34;, job_definition_name=f\u0026#39;{JOB_DEFINITION_NAME}\u0026#39;, timeout={ \u0026#39;attemptDurationSeconds\u0026#39;: 900, }, # type (so this is not multi-node) type=\u0026#39;container\u0026#39;, platform_capabilities=[\u0026#39;FARGATE\u0026#39;], # container properties container_properties=batch.CfnJobDefinition.ContainerPropertiesProperty( # THIS IS \u0026#34;SOME\u0026#34; OF THE FARGATE PLATFORM CONFIGURATION. THE REASON I SAID SOME IS BECAUSE, NOT SURE WHY, THE SETTINGS ARE IN MANY PLACES # assign public IP (does not found) # Ephemeral storage ephemeral_storage=batch.CfnJobDefinition.EphemeralStorageProperty( size_in_gib=30 # between 21 and 200 ), # Execution role execution_role_arn=ecs_role.role_arn, # ++++ THIS IS SOME MORE FARGATE CONFIGURATION +++++ # This is the Latest that you see in the console fargate_platform_configuration= batch.CfnJobDefinition.FargatePlatformConfigurationProperty( platform_version=\u0026#39;LATEST\u0026#39;, ), # network configuration (For public IP) network_configuration=batch.CfnJobDefinition.NetworkConfigurationProperty( assign_public_ip=\u0026#39;ENABLED\u0026#39;, ), # privileged: Do not add this!!! Fargate actually ban this # privileged=True, # THIS IS ON THE TAB CONTAINER CONFIGURATION IN JOB DEFINITION (Step 2 if you use Console) # This is the link to the ECR that we store our image (the image that we build, and create in the CICD step). Don\u0026#39;t forget to add the tag image=f\u0026#39;{ECR_REPO_NAME}:latest\u0026#39;, # Honestly, this is not really useful, since I have already set the ENTRYPOINT in the Dockerfile command=[ \u0026#39;echo\u0026#39;, \u0026#39;Job Definition Initiated\u0026#39;, ], # job role: Explain simply, Job role is more specific than Execution role. All the job can have the same execution role, but different job role job_role_arn=ecs_role.role_arn, # memory and vcpus resource_requirements=[ batch.CfnJobDefinition.ResourceRequirementProperty( type=\u0026#39;MEMORY\u0026#39;, value=\u0026#39;2048\u0026#39;, ), batch.CfnJobDefinition.ResourceRequirementProperty( type=\u0026#39;VCPU\u0026#39;, value=\u0026#39;1\u0026#39;, ), ], ), ) # +++ Create a Lambda to submit job to Batch +++ # create a role for lambda to submit job to batch submit_job_lambda_role = iam.Role( scope=self, id=\u0026#34;SubmitBatchJobLambdaRole\u0026#34;, role_name=\u0026#34;SubmitJobLambdaRole\u0026#34;, assumed_by=iam.ServicePrincipal(\u0026#34;lambda.amazonaws.com\u0026#34;), # add the job submit policy to the role managed_policies=[ iam.ManagedPolicy.from_aws_managed_policy_name( managed_policy_name=\u0026#34;service-role/AWSBatchServiceEventTargetRole\u0026#34; ), ], ) # create a lambda function submit_job_lambda = aws_lambda.Function( scope=self, id=\u0026#34;SubmitJobLambda\u0026#34;, function_name=\u0026#34;CDK-JobScraping-Submit-Job-Batch\u0026#34;, runtime=aws_lambda.Runtime.PYTHON_3_9, code=aws_lambda.Code.from_asset(\u0026#34;workshop1/lambda_functions/TriggerJobScrapingTask/\u0026#34;), handler=\u0026#34;lambda_function.lambda_handler\u0026#34;, timeout=Duration.seconds(60), # pass in the role role=submit_job_lambda_role, environment={ \u0026#34;JOB_NAME\u0026#34;: JOB_NAME, \u0026#34;JOB_QUEUE\u0026#34;: JOB_QUEUE_NAME, \u0026#34;JOB_DEFINITION\u0026#34;: JOB_DEFINITION_NAME, }, ) # +++ Event Bridge Rule +++ # create a rule to trigger the lambda function at 6am everyday rule = events.Rule( scope=self, id=\u0026#34;TriggerSubmitJobLambda\u0026#34;, rule_name=\u0026#34;TriggerSubmitJobLambda\u0026#34;, schedule=events.Schedule.cron( minute=\u0026#34;0\u0026#34;, hour=\u0026#34;6\u0026#34;, ), ) # add the lambda function as the target rule.add_target(targets.LambdaFunction(submit_job_lambda)) "
},
{
	"uri": "/vi/4-result/",
	"title": "Kết quả",
	"tags": [],
	"description": "",
	"content": "Kết quả Chúc mừng bạn. Vậy là bạn đã đến được bước cuối cùng rồi. Việc còn lại là tận hưởng thành quả thôi\nVào lại Ec2, chọn Load Balancer. Copy DNS name từ Load Balancer vừa tạo Nó sẽ trông như thế này CDKJobScrapingFrontendLB-1591468448.us-east-1.elb.amazonaws.com Thêm cổng 8501 vào phía sau CDKJobScrapingFrontendLB-1591468448.us-east-1.elb.amazonaws.com:8501\nNếu bạn ra được trang như thế này thì chúc mừng bạn đã thành công "
},
{
	"uri": "/vi/6-cdk/6.3-wrapup/",
	"title": "Thiết kế frontend Webapp",
	"tags": [],
	"description": "",
	"content": "Theo mình thì đây là phần khó nhất trong cả 3 phần khi làm bằng CDK. Lý do là vì phần này rất dài. Sau đây là tổng quan về những gì mình sẽ làm trong phần này:\nThiết kế hạ tầng mạng Đoạn code dưới tạo 1 VPC với 2 subnet, 1 public và 1 private. Khi tạo Public Subnet bằng code thì AWS tự route tới Internet Gateway cho mình. Và khi tạo Private Subnet loại PRIVATE_WITH_NAT thì AWS tự tạo NAT Gateway cho mình.\n# Create VPC vpc = ec2.Vpc( scope=self, id=\u0026#39;CDKJobScrapingFrontend\u0026#39;, vpc_name=f\u0026#39;{VPC_NAME}\u0026#39;, # set the CIDR for the VPC cidr= \u0026#39;10.0.0.0/16\u0026#39;, # enable DNS support enable_dns_support=True, # setup 2 subnet subnet_configuration=[ # public subnet ec2.SubnetConfiguration( name=\u0026#34;CDKJobScrapingFrontendPublic\u0026#34;, subnet_type=ec2.SubnetType.PUBLIC, cidr_mask=24, ), # private subnet with NAT ec2.SubnetConfiguration( name=\u0026#34;CDKJobScrapingFrontendPrivate\u0026#34;, subnet_type=ec2.SubnetType.PRIVATE_WITH_NAT, cidr_mask=24, ), ], ) Tạo Security Group cho EC2 Instance và Bastion Host # +++ Create Ec2 +++ # create a security group for the ec2 private instance private_ec2_security_group = ec2.SecurityGroup( scope=self, id=\u0026#34;CDKJobScrapingFrontendSecurityGroup\u0026#34;, vpc=vpc, allow_all_outbound=True, description=\u0026#34;Security Group for the EC2 private instance\u0026#34;, security_group_name=\u0026#34;CDK-JobScraping-Frontend-Private-SG\u0026#34;, ) # add the ingress rule to the security group, open custom TPC port 8501 private_ec2_security_group.add_ingress_rule( peer=ec2.Peer.any_ipv4(), connection=ec2.Port.tcp(8501), description=\u0026#34;Allow inbound traffic from anywhere for port 8501\u0026#34;, ) # add ssh port 22 for for SG for the EC2 private instance private_ec2_security_group.add_ingress_rule( peer=ec2.Peer.any_ipv4(), connection=ec2.Port.tcp(22), description=\u0026#34;Allow inbound traffic from anywhere for port 22\u0026#34;, ) # create security group for bastion host bastion_security_group = ec2.SecurityGroup( scope=self, id=\u0026#34;CDKJobScrapingFrontendBastionSecurityGroup\u0026#34;, vpc=vpc, allow_all_outbound=True, description=\u0026#34;Security Group for the bastion host\u0026#34;, security_group_name=\u0026#34;CDK-JobScraping-Frontend-Bastion-SG\u0026#34;, ) # add the ingress rule to the security group, open SSH port 22 bastion_security_group.add_ingress_rule( peer=ec2.Peer.any_ipv4(), connection=ec2.Port.tcp(22), description=\u0026#34;Allow inbound traffic from anywhere for port 22\u0026#34;, ) Tạo Bastion Host Lưu ý: do mình không tìm được cách download key pem khi dùng CDK, nên bạn sẽ phải tạo sẵn 1 key pem và lưu tên vào biến môi trường nhé.\n# create a bastion host # AMI for bastion host ami_bastion_host = ec2.MachineImage.generic_linux( { \u0026#34;us-east-1\u0026#34;:\u0026#34;ami-053b0d53c279acc90\u0026#34; } ) # DON\u0026#39;T FORGET THAT YOU WILL NEED TO CREATE A KEY PAIR IN THE AWS CONSOLE (BECAUSE YOU COULDN\u0026#39;T ACCESS IT LATTER IF CREATE HERE) bastion_host = ec2.Instance( scope=self, id=\u0026#34;CDKJobScrapingFrontendBastionHost\u0026#34;, instance_type=ec2.InstanceType(\u0026#34;t2.micro\u0026#34;), machine_image=ec2.MachineImage.latest_amazon_linux( generation=ec2.AmazonLinuxGeneration.AMAZON_LINUX_2 ), vpc=vpc, vpc_subnets=ec2.SubnetSelection( subnet_type=ec2.SubnetType.PUBLIC ), security_group=bastion_security_group, key_name=f\u0026#39;{KEY_NAME}\u0026#39;, instance_name=\u0026#39;CDKJobScrapingFrontendBastionHost\u0026#39;, ) Tạo ASG và ALB Trong đoạn code dưới, mình đã:\nTạo 1 role cho ASG Tạo Launch Instance. Cần gán key pem nếu như muốn truy cập vào instance sau này. Launch Instance cũng cần SG. Đồng thời cần gán User Data cho Launch template Tạo ASG Tạo ALP Tạo Target Group và gán ASG vào Target Group # +++ Create Load Balancer and Auto Scaling Group +++ # But first, we will need 2 things # 1. Create an AMI that have all the packages installed # 3. Create a Role for ASG # Since the first steps would be done in console, I\u0026#39;ll do the 3rd step here # Create a role for ASG role = iam.Role( scope=self, id=\u0026#39;CDKJobScrapingFrontendRole\u0026#39;, assumed_by=iam.ServicePrincipal(\u0026#39;ec2.amazonaws.com\u0026#39;), role_name=\u0026#39;CDKJobScrapingFrontendRole\u0026#39;, managed_policies=[ # Read access to Ec2 iam.ManagedPolicy.from_aws_managed_policy_name(\u0026#39;AmazonEC2ReadOnlyAccess\u0026#39;), # Full access to S3 iam.ManagedPolicy.from_aws_managed_policy_name(\u0026#39;AmazonS3FullAccess\u0026#39;), ] ) # Okay, now let move on to create the Load Balancer and Auto Scaling Group # Create Launch Template. Don\u0026#39;t Forget a Key Pair launch_template = ec2.LaunchTemplate( scope=self, id=\u0026#39;CDKJobScrapingFrontendLaunchTemplate\u0026#39;, launch_template_name=\u0026#39;CDKJobScrapingFrontendLaunchTemplate\u0026#39;, # set the machine image from the AMI that we created machine_image=ec2.MachineImage.generic_linux( { \u0026#34;us-east-1\u0026#34;:f\u0026#39;{AMI}\u0026#39; } ), # set the instance type instance_type=ec2.InstanceType(\u0026#34;t2.micro\u0026#34;), # set the key pair key_name=f\u0026#39;{KEY_NAME}\u0026#39;, # set the security group security_group=private_ec2_security_group, # set the role role=role, # set the block device mapping: use AMI block device mapping user_data=ec2.UserData.custom( \u0026#39;\u0026#39;\u0026#39;Content-Type: multipart/mixed; boundary=\u0026#34;//\u0026#34; MIME-Version: 1.0 --// Content-Type: text/cloud-config; charset=\u0026#34;us-ascii\u0026#34; MIME-Version: 1.0 Content-Transfer-Encoding: 7bit Content-Disposition: attachment; filename=\u0026#34;cloud-config.txt\u0026#34; #cloud-config cloud_final_modules: - [scripts-user, always] --// Content-Type: text/x-shellscript; charset=\u0026#34;us-ascii\u0026#34; MIME-Version: 1.0 Content-Transfer-Encoding: 7bit Content-Disposition: attachment; filename=\u0026#34;userdata.txt\u0026#34; #!/bin/bash # Run the app.py script cd /home/ubuntu/ sudo streamlit run streamlit-app.py \u0026gt; output.txt 2\u0026gt;\u0026amp;1 --//--\u0026#39;\u0026#39;\u0026#39; ), ) # Create Auto Scaling Group auto_scaling_group = autoscaling.AutoScalingGroup( scope=self, vpc=vpc, id=\u0026#39;CDKJobScrapingFrontendAutoScalingGroup\u0026#39;, # set the name for the Auto Scaling Group auto_scaling_group_name=\u0026#39;CDKJobScrapingFrontendAutoScalingGroup\u0026#39;, # set launch template launch_template = launch_template, # VPC private subnet vpc_subnets=ec2.SubnetSelection( subnet_type=ec2.SubnetType.PRIVATE_WITH_NAT # current we since we only have 2 private sbunet, so can use this to select all ), # set the min, max, and desired capacity min_capacity=0, max_capacity=2, desired_capacity=1, ) # create a load balancer load_balancer = elbv2.ApplicationLoadBalancer( scope=self, id=\u0026#39;CDKJobScrapingFrontendLoadBalancer\u0026#39;, vpc=vpc, internet_facing=True, load_balancer_name=\u0026#39;CDKJobScrapingFrontendLB\u0026#39;, security_group=private_ec2_security_group, ) # create a target group that target to the Auto Scaling Group target_group = elbv2.ApplicationTargetGroup( scope=self, id=\u0026#39;CDKJobScrapingFrontendTargetGroup\u0026#39;, vpc=vpc, port=8501, protocol=elbv2.ApplicationProtocol.HTTP, targets=[auto_scaling_group], target_group_name=\u0026#39;CDKJobScrapingFrontendTG\u0026#39;, ) # create a listener that listen to the target group listener = load_balancer.add_listener( id=\u0026#39;CDKJobScrapingFrontendListener\u0026#39;, port=8501, protocol=elbv2.ApplicationProtocol.HTTP, open=True, default_target_groups=[target_group], ) "
},
{
	"uri": "/vi/5-cleanup/",
	"title": "Dọn dẹp tài nguyên",
	"tags": [],
	"description": "",
	"content": "Giới thiệu tổng quát Sau khi hoàn thành workshop. Bạn cần xoá đi tài nguyên để tránh phát sinh thêm chi phí.\nXoá ASG Vào lại Ec2, chọn Auto Scaling Groups Chọn ASG bạn đã tạo và chọn Action -\u0026gt; Delete. Nhập chữ delete để xác nhận Xoá Load Balancer Vào lại EC2, chọn Load Balancers (trên Auto Scaling Groups 1 xíu, ngay trên target groups) Chọn Load Balancer bạn đã tạo và chọn Action -\u0026gt; Delete Load Balancer. Gõ confirm -\u0026gt; Delete Xoá Target Group Vào lại EC2, chọn Target Groups (trên Auto Scaling Groups) Chọn Target Group bạn đã tạo và chọn Action -\u0026gt; Delete. Chọn Yes, delete Xoá những Instane Vào Ec2, chọn Instances Chọn những instance đang chạy và chọn Instance state -\u0026gt; Terminate. Xoá AWS Batch Job Definition Vào AWS Batch, chọn tab Job definitions Chọn Job Definition bạn đã tạo và chọn Deregister Xoá AWS Batch Job Queue Vào AWS Batch, chọn tab Job queues Chọn Job Queue bạn đã tạo và chọn Disable Sau khi disable, chọn lại queue và chọn Delete Xoá AWS Batch Compute Environment Vào AWS Batch, chọn tab Compute environments Chọn Compute Environment bạn đã tạo và chọn Disable Chọn lại Compute Environment và chọn Delete (phải đợi 1 lúc cho đến khi job queue xoá xong thì mới xoá được compute environment) Xoá Codebuild Project Vào AWS Codebuild Chọn Project bạn đã tạo và chọn Delete build project Nhập delete để confirm, chọn Delete Xoá AWS ECR Repository Vào AWS ECR Chọn Repository bạn đã tạo và chọn Delete Nhập delete để confirm, chọn Delete Xoá VPC Vào VPC Chọn những VPC bạn đã tạo, Chọn Action -\u0026gt; Delete VPC Nhập delete để confirm, chọn Delete "
},
{
	"uri": "/vi/6-cdk/6.3-frontend/",
	"title": "Hoàn tất, Chạy CDK và Dọn dep",
	"tags": [],
	"description": "",
	"content": "Vậy là bạn hoàn thành được phần bonus của workshop này rồi. Sau khi hiểu các khái niệm cơ bản và các hoạt động của từng file. Bạn có thể bắt đầu tạo hệ thống bằng CDK rồi.\n1. Đảm bảo rằng bạn đã chuẩn bị đầy đủ Hãy vào file .env và thay đổi tuỳ vào môi trường của bạn. Đây thường là những tài nguyên mình không tạo bằng CDK được. Hãy chắc rằng mình đã login vào AWS và vào tài khoản github chứa code crawler của bạn 2. Chạy CDK Bạn có thể chạy lệnh sau trong IDE để vào môi trường CDK source .venv/bin/activate Sau khi vào môi trường CDK, bạn có thể chạy lệnh sau để cài đặt các thư viện cần thiết $ pip install -r requirements.txt Sau đó, bạn có thể synth ra các file Cloudformation bằng lệnh sau: cdk synth Nếu như được yêu cầu bootstrap, bạn có thể chạy lệnh sau: cdk bootstrap Cuối cùng là deploy lên bằng lệnh sau: cdk deploy --all để chạy tất cả stack 3. Dọn dẹp Sau khi chạy xong, nếu muốn xoá tất cả các tài nguyên, bạn có thể chạy lệnh sau: cdk destroy --all, Nhanh hơn rất nhiều so với thủ công đúng không\n"
},
{
	"uri": "/vi/6-cdk/",
	"title": "Xây dựng workshop bằng CDK",
	"tags": [],
	"description": "",
	"content": "Giới thiệu tổng quát Đây là phần bonus của workshop này. Bạn sẽ được hướng dẫn xây dựng lại workshop này bằng CDK.\nSơ lược về CDK: Đây là 1 dịch vụ của AWS, cho phép xây dựng và triển khai các tài nguyên AWS bằng code. CDK hỗ trợ nhiều ngôn ngữ như Python, Javascript, Typescript, Java, C#,\u0026hellip; Trong bài workshop này thì mình sẽ dùng CDK với Python. CDK tạo các stack trong Cloudformation để triển khai tài nguyên. Nên nếu gặp lỗi hay cần giám sát, bạn có thể vào AWS Cloudformation.\nChuẩn bị Đầu tiên, bạn cần clone github repo chứa code CDK về. Bạn có thể dùng lệnh sau: git clone -b cdk https://github.com/MinhThieu145/FCJBootcamp.git Sau đó, bạn cần setup AWS CLI để tương tác với Resource. Nếu bạn sử dụng pip thì có thể sử dụng lệnh sau để install AWS CLI: pip install awscli. Để kiểm tra thì bạn có thể dùng lệnh: aws --version\nSau đó, bạn cần đăng nhập vào tài khoản AWS của mình. Bạn có thể dùng lệnh sau: aws configure. Lệnh này sẽ yêu cầu bạn nhập Access Key và Secret Key. Bạn có thể lấy 2 key này ở phần IAM trên AWS Console.\nVào AWS IAM, chọn User và chọn Add User\nChọn User name -\u0026gt; Next\nTrong tab Permission, chọn Attach policies directly -\u0026gt; Tìm và chọn AdministratorAccess -\u0026gt; Next\nChọn Create user\nVào lại IAM -\u0026gt; User và chọn user bạn vừa tạo. Chọn vào tab Security Credential Kéo xuống phần Access Key và chọn Create access key\nChọn Use case là Command Line Interface (CLI) -\u0026gt; Tick confirmation -\u0026gt; Next\nChọn Create access key\nBên cạnh nút Done chọn Download .csv. File này sẽ chứa Access Key và Secret Key của bạn. Bạn cũng có thể copy trực tiếp Access Key và Secret access key từ console Vào lại IDE của bạn, mở terminal lên và chạy lệnh aws configure. Nếu đã cài CLI thì sẽ có prompt hỏi access key và Secret access key. Paste vào là xong. Bạn cũng có thể chọn region đúng với region đang sử dụng Vậy là bạn đã hoàn thành việc đang nhập vào tài khoản AWS của mình. Bây giờ bạn có thể chạy CDK được rồi.\n"
},
{
	"uri": "/vi/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/vi/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]